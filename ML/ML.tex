%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Simple Sectioned Essay Template
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing essay content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article} % Default font size is 12pt, it can be changed here

\usepackage{geometry} % Required to change the page size to A4
\geometry{a4paper} % Set the page size to be A4 as opposed to the default US Letter

\usepackage{graphicx} % Required for including pictures
\usepackage{amsmath}
\usepackage[ampersand]{easylist}
\usepackage{import}
\usepackage{listings}
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}


\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}




\usepackage{float} % Allows putting an [H] in \begin{figure} to specify the exact location of the figure
\usepackage{wrapfig} % Allows in-line images such as the example fish picture
\usepackage{bm}


\linespread{1.2} % Line spacing

%\setlength\parindent{0pt} % Uncomment to remove all indentation from paragraphs

\graphicspath{{Pictures/}} % Specifies the directory where pictures are stored

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
\HRule \\[0.5cm]
{ \LARGE  MACHINE LEARNING - FORMULAS}\\[0.8cm] % Title of your document
\HRule \\[1.5cm]
\textsc{\large by Yannick Giovanakis}\\[5.5cm] % Minor heading such as course title

\vfill
{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise
\end{titlepage}

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\tableofcontents % Include a table of contents

\newpage % Begins the essay on a new page instead of on the same page as the table of contents 

\section{Linear Regression}
\begin{itemize}
\item Simple linear regression with D-dimensional input vector \textbf{x}
$$ y(\bm{x,b}) = w_0 + \sum \limits_{j=1}^{D-1}w_jx_j = \bm{w}^T \bm{x}$$

\item Linear regression with M fixed non-linear functions ( \textbf{basis functions})
$$ y(\bm{x,b}) = w_0 + \sum \limits_{j=1}^{M-1}w_j \phi_j(\bm{x}) = \bm{w}^T \phi (\bm{x})$$
\begin{itemize}
\item Polynomial :$\phi_j(x) = x^j$
\item Gaussian : $\phi(x) = e^{-\frac{(x-\mu_j)^2}{2 \sigma ^2}}$
\item Sigmoid : $\phi(x) = \frac{1}{1+e^{\frac{(\mu_j - x)}{\sigma}}}$
\end{itemize}
\end{itemize}

\subsection{Direct Approach - OLS}
\begin{itemize}
\item Half of \textbf{residual sum of squares} RSS/SSE 
$$ L(w) = \frac{1}{2} \sum \limits_{n=1}^{N} (y(x_n , \bm{w}) -t_n)^2$$
$$ RSS(\bm{w}) = || \epsilon||^2_2 = \sum \limits^N \epsilon_i^2$$

\item RSS in matrix form and $\epsilon = \bm{t} -\bm{\phi w}$
$$ L(w)  = \frac{1}{2} RSS(\bm{w}) = \frac{1}{2}( \bm{t} - \bm{\phi w})^T(\bm{t} - \bm{\phi w})$$

\item Minimize LS 
$$ \frac{\partial L(w)}{\partial w} = - \bm{\phi}^T(\bm{t} - \bm{\phi w}) \quad 
   \frac{\partial^2 L(w)}{\partial w \partial w^T} = \bm{\phi}^T \bm{\phi} $$

\begin{itemize}
\item If $\bm{\phi}^T \bm{\phi}$ \textbf{singular} $\rightarrow$ infinite solutions
\item If $\bm{\phi}^T \bm{\phi}$ \textbf{non - singular} , all \textbf{eigenvalues} $ \geq 0 $ :
$$ - \bm{\phi}^T(\bm{t}- \bm{\phi w}) = 0 \rightarrow \bm{\phi ^T \phi w} = \bm{\phi^T t} $$
$$ \hat{\bm{w}}_{OLS} = (\bm{\phi^T \phi })^{-1} \bm{\phi^T t}$$
$$ \hat{\bm{t}} = \bm{\phi \hat{w}} = \bm{\phi (\phi^T \phi)}^{-1} \bm{\phi^T t}$$
\[\underbrace{\bm{\phi (\phi^T \phi)}^{-1} \bm{\phi^T}}_{\text{Hat matrix}} \]
\end{itemize}
\end{itemize}

\subsection{Direct Approach - Gradient Optimization}
\begin{itemize}
\item Weight update with \textbf{stochastic gradient descent} and learning rate $\alpha$
$$ \bm{w}^{(k+1)} = \bm{w}^{(k)} - \alpha^{(k)}\nabla L(X_n)$$
$$ \bm{w}^{(k+1)} = \bm{w}^{(k)} - \alpha^{(k)} (\bm{w}^{(k)^T \phi}(x_n) - \bm{t_n}) \bm{\phi}(x_n)$$

\item Condition 1 for convergence $\sum \limits_{k=0}^{\infty} \frac{1}{\alpha^{(k)}} = + \infty$
\item Condition 2 for convergence $\sum \limits_{k=0}^{\infty} \frac{1}{\alpha^{(k)^2}} < + \infty$
\end{itemize}

\subsection{Discriminative approach - MLE}
\begin{itemize}
\item Target t given by \textbf{deterministic} function y with a \textbf{Gaussian noise} $ \epsilon \sim \mathcal{N}(0,\sigma^2)$
$$ t= y(\bm{x,w}) + \epsilon$$
So that $t \sim \mathcal{N}(y(x,w), \sigma^2)$

\item Given N samples i.i.d  and outputs , the \textbf{likelihood} is :
$$ p(\bm{t} | \bm{X,w},\sigma^2) = \prod \limits_{n=1}^N \mathcal{N}(t_n| \bm{w}^T \bm{\phi}(x_n),\sigma^2 ) = \prod \limits_{n=1}^N \frac{1}{\sqrt{2 \pi \sigma^2}}e^{- \frac{(t-y(x,w))^2}{2\sigma^2}}$$

\item Use  \textbf{w} to approximate Gaussian mean using \textbf{MLE} (log-likelihood)
 $$ l(\bm{w}) = ln p(\bm{t}| \bm{X,w},\sigma^2) = \sum \limits_{n=1}^N ln p(t_n | x_n , \bm{w} , \sigma^2)=$$
 $$ -\frac{N}{2}ln(2\pi \sigma^2) - \frac{1}{2\sigma^2}RSS(w) = -\frac{N}{2}ln(2\pi \sigma^2)- \frac{1}{4\sigma^2}(\bm{t-w \phi})^T(t-w\phi)$$
$$ \nabla l(\bm{w})= -\bm{\phi}^T (\bm{t- \phi w})=  -\bm{\phi^T \phi} + \bm{\phi^T \phi w}= 0$$
$$ \bm{w}_{ML} = (\bm{\phi^T \phi})^{-1} \bm{\phi}^T t$$
$$\text{Assuming Gaussian distribution } \hat{w}_{ML} = \hat{w}_{OLS})$$

\item Case of \textbf{multiple outputs} 
$$ \hat{\bm{W}}_{ML} = (\bm{\phi^T \phi}^{-1} \bm{\phi^T T}$$
$$ \hat{\bm{w}}_{ML} = (\bm{\phi^T \phi}^{-1} \bm{\phi^T t}_k)$$
\end{itemize}

\subsection{Regularization}
\begin{itemize}

\item Empirical loss on data ($L_D$) + size of parameters ($L_W$):
$$ L(w) = L_D(w) + \lambda L_W(w)$$

\item \textbf{Ridge Regression} with l2-norm
$$ L_W(w) = \frac{1}{2}\bm{w^T w}= \frac{1}{2}||\bm{w}||^2_2$$
$$ L(\bm{w}) = \frac{1}{2} \sum \limits_{i=1}^{N} (t_i -\bm{w}^T \phi(x_i))^2 + \frac{\lambda}{2}||w||^2_2$$
\begin{itemize}
\item Closed form solution:  $\hat{\bm{w}}_{ridge} = (\lambda \bm{I} + \bm{\phi^T \phi})^{-1} \phi^T t$
\end{itemize}

\item \textbf{Lasso Regression} with l1-norm
$$ L_W(w) = \frac{1}{2}\bm{w}= \frac{1}{2}||\bm{w}||_1$$
$$ L(\bm{w}) = \frac{1}{2} \sum \limits_{i=1}^{N} (t_i -\bm{w}^T \phi(x_i))^2 + \frac{\lambda}{2}||w||_1$$
\begin{itemize}
\item \textbf{No} closed-form solution (it's non-linear)
\end{itemize}
\end{itemize}

\subsection{Bayesian Regression}
\begin{itemize}
\item Posterior distribution $\propto$ likelihood $\cdot$ prior
$$ p(\bm{w}|D) =\frac{p(D| \bm{w})p(\bm{w})}{p(D)}$$
$$ p(D) = \int p(D|\bm{w})p(\bm{w}) dw \rightarrow \text{\textbf{ normalizing constant }}$$

\item Obtain most probable value for $w$ given the data using \textbf{Maximum a Posteriori} which is the \textbf{mode} of $p(\bm{w}|D)$

\item Prediction of new point $x^*$ given data D (predictive distribution)
$$ p(x^*|D) = \int p(x^*|\bm{w},D)p(\bm{w}|D)dw = E[p(x^* |\bm{w},D)]$$

\item Assuming \textbf{Gaussian} likelihood, conjugate prior is also \textbf{Gaussian}
\begin{itemize}
\item Prior : $p(\bm{w}) = \mathcal{N}(\bm{w}|\bm{w_0}m,S_0)$
\item Posterior : $p(\bm{w}|\bm{t,\Phi},\sigma^2) \propto \mathcal{N}(\bm{w}|\bm{w_0,S_0}) \mathcal{N}(\bm{t}|\bm{\Phi w},\sigma^2 \bm{I_N})) = \mathcal{N}(\bm{w}|\bm{w_N,S_N})$
\item $\bm{w}_N = \bm{S}_N \left( \bm{S}_0^{-1}\bm{w}_0 + \frac{\bm{\phi^T t}}{\sigma^2}  \right) \rightarrow$ \textbf{MAP Estimator}
\item $\bm{S_N}^{-1} = \bm{S_0}^{-1} + \frac{\bm{\Phi^T \Phi}}{\sigma^2}$
\end{itemize}

\item Degeneration to ML if $\bm{S}_0 \rightarrow \infty$ 
$$ \bm{S_N}^{-1} = 0 + \frac{\bm{\Phi^T \Phi}}{\sigma^2}$$
$$ \bm{S_N} = \sigma^2 (\bm{\phi^T \phi})^{-1}$$
$$ \bm{w_N} = \sigma^2 (\bm{\phi^T \phi})^{-1} \frac{\bm{\Phi^T t}}{\sigma^2} = (\bm{\phi^T \phi})^{-1} \bm{\phi^T t}  $$

\item Degeneration to Ridge if $\bm{w_0}=0 , \bm{S_0} =\tau^2\bm{I}$
$$ \lambda = \frac{\sigma^2}{\tau^2}$$

\item \textbf{Posterior predictive distribution} takes into account all the models
$$ p(\bm{t} | \bm{x,D} ,\sigma^2) = \int \mathcal{N} (\bm{t}| \bm{w}^T \phi(x),\sigma^2)\mathcal{N}(\bm{w}| \bm{w_N, S_N})d\bm{w}$$
$$ = \mathcal{N}(t| \bm{w_N}^T \phi(\bm{x},\sigma^2_N(\bm{x})))$$
$$ \sigma^2_N(\bm{x}) = \underbrace{\sigma^2}_{noise in target values} + \underbrace{\phi(\bm{x})^T\bm{S_}N \bm{\phi(x)}}_{uncertainty with parameter values}$$
$$\text{if \textbf{N}} \rightarrow \infty  : \bm{\phi}(\bm{x})^T\bm{S}_N \bm{\phi}(\bm{x}) \rightarrow 0 $$
$$ \text{Variance depends only on  } \sigma^2$$ 
\end{itemize}

\section{Classification}
\begin{itemize}
\item Function f can be \textbf{non-linear} so non-linear in parameters but linear in \textbf{decision surfaces}
 $$ y(\bm{x},\bm{w})= f( \bm{x}^T \bm{w} + w_0)  $$
 
\item Approaches:
\begin{itemize}
\item \textbf{Discriminant} : (direct approach) build function that directly maps input to class
\item \textbf{Probabilistic discriminative} : model conditional probability $p(C_k|x)$ directly using parametric models
\item \textbf{Probabilistic generative} : mode $p(x|C_k)$ , class conditional density , and $p(C_k)$ then use Bayes' Rule
\end{itemize}
\end{itemize}

\subsection{Discriminant approach}

\subsubsection{Considerations}
\begin{itemize}
\item Two class problem with model $y(x) = \bm{x}^T \bm{w} + w_0$
\item Assign $C_1$ if $y(x) \geq 0$ , $C_2$ otherwise
\item Leads to decision boundary $y(x) = 0$  
\end{itemize}
\newpage
\textbf{Direction of decision boundary}
\begin{itemize}
\item  Two points on surface $x_A$ , $x_B$
$$ y(x_A)=y(x_B) = 0$$
$$ \begin{cases}
x_A^T \bm{w}+w_0 =0\\
x_B^T \bm{w}+w_0 =0
\end{cases}
$$
The difference vector identifies the decision boundary and \textbf{w} is \textbf{orthogonal} to it (because scalar product is 0)
$$ (x_A - x_B) \bm{w}=0$$
This means that \textbf{w} changes the \textbf{direction} of the decision boundary.
\end{itemize}
\textbf{Location of decision boundary}
\begin{itemize}
\item Point x on decision boundary 	
$$ d(0,y(x) = \frac{|\bm{0} \cdot y(x)|}{||\bm{w}||}= \frac{|\sum 0 \cdot w_i + w_0|}{||\bm{w}||} = \frac{w_0}{||\bm{w}||}$$
\end{itemize}
Which means that the \textbf{location} is given by the bias term $w_0$

\subsubsection{Multi-class problem}
\begin{itemize}
\item \textbf{One-versus-the-rest} : $K-1$ classifiers (each solves a 2 class problem)
\item \textbf{One-versus-one} : $\frac{K(K-1)}{2}$ classifiers
\item  Using \textbf{k-linear discriminant} functions: $y_k(\bm{x}) = \bm{x}^T \bm{w}_k + w_{k0} $.\\
In this case the decision boundary between k,j is $y_k(x) = y_j(x)$.
\end{itemize}


\subsubsection{Least Squares for classification}
\begin{itemize}
\item General model with 1-of-k encoding for target t
$$ y_k = \bm{x}^T \bm{w}_k + w_{k0} = \bm{\tilde{W}}^T \bm{\tilde{x}}$$
$$ \tilde{W} = \begin{bmatrix}
w_{0,1} & ... & w_{0,k} \\
... & ... & ... \\
w_{D,1} & ... & w_{D,k}
\end{bmatrix}  \quad \tilde{x} = \begin{bmatrix}
1 \\ x_0 \\ ... \\x_D
\end{bmatrix} $$
\item Aim : find optimal weight matrix $\tilde{W}$ (same as OLS)
$$ \bm{\tilde{W}} = (\bm{\tilde{X}^T \tilde{X}})^{-1} \bm{\tilde{X}}^T \bm{T} $$
$$ \text{Assign input to class for which }t_k = \tilde{\bm{x}}^T \tilde{\bm{w}}_k \text{ is largest}$$

\item Method very bad because of \textbf{outliers}
\end{itemize}

\subsubsection{Perceptron}
\begin{itemize}
\item On-line algorithm with model
$$ y(x) = f(\bm{w}^T \phi(x))$$
$$ f = \begin{cases}
+1 \quad a \geq 0\\
-1 \quad a < 0
\end{cases}
$$
\item Finds hyperplane by minimizing distance of \textbf{missclassified points} to boundary
$$ L_P(x) = - \sum \limits_{n \in M }\bm{w}^T \phi(x_n)t_n $$
$$ M = \text{ set of missclassified points}$$

\item Minimizing with \textbf{stochastic gradient descent}
$$ w^{(k+1)} = w_{(k)}-\alpha \nabla L_P(w)= w^{(k)} + \alpha \phi(x_n)t_n$$
\end{itemize}

\subsection{Probabilistic Discriminative Approach}
\begin{itemize}
\item In \textbf{logistic regression} the \textbf{posterior probability} of class $C_1$ can be written as \textbf{logistic sigmoid function} 
$$ p(C_1|\phi) = \frac{1}{e^{-\bm{w}^\phi}} = \sigma(\bm{w}^ \sigma)$$
$$ p(C_2|\phi) = 1- p(C_1|\phi)$$

\item Maximize probability of getting right label by using ML
$$ y_n = \sigma(\bm{w}^T \phi_n)$$ 
$$ p(\bm{t}|\bm{X,w}) = \prod \limits_{n=1}^N y_n^{t_n}(1-y_n)^{1-t_n}$$
$$ L(w) = - ln p(\bm{t}|\bm{X,w})= - \sum \limits_{n=1}^N (t_n ln(y_n)+(1-t_n)ln(1-y_n))= \sum \limits_{n=1}^N  L_n $$

\item Minimize now $L_n$
$$ \frac{\partial L_n}{\partial y_n} = \frac{y_n-t_n}{y_n(1-t_n)}$$
$$ \frac{\partial y_n}{\partial \bm{w}} = y_n(1-y_n)\phi_n$$
$$ \frac{\partial L_n}{\partial \bm{w}} = \frac{\partial L_n}{\partial y_n} \cdot \frac{\partial y_n}{\partial \bm{w}} = (y_n-t_n)\phi_n$$
$$ \nabla L(\bm{w}) = \sum \limits_{n=1}^N (y_n - t_n)\phi_n$$
\end{itemize}


\subsubsection{Multi-class using Softmax}
\begin{itemize}
\item Softmax transformation of linear functions of feature variables
$$ p(C_k|\phi) = y_k(\phi)= \frac{e^{\bm{w}^T_{k}}\phi}{\sum \limits_j e^{\bm{w}^T_j \phi}}$$
\item Determine parameters using ML (differently done in \textbf{generative approaches})
$$ p(\bm{T} |\bm{\Phi ,w_1,...,w_k}) = \prod \limits_{n=1}^N \left(\underbrace{ \prod \limits_{k=1}^K p(C_k |\Phi_n)^{t_{nk}}}_{\text{only one term corresponding to right class}} \right) = \prod\limits_{n=1}^{N} \left( \prod \limits_{k=1}^K  y_{nk}^{t_{nk}}\right)$$
where $y_{nk} = p(C_k|\phi_n)= \frac{e^{\bm{w}^T_{k}}\phi}{\sum \limits_j e^{\bm{w}^T_j \phi}}$

\item Minimize with negative logarithm to get the \textbf{cross-entropy} function
$$ L(\bm{w_1,...,w_k}) = - ln(p(\bm{T}| \bm{\Phi,w_1,...,w_k}))= -\sum
\limits_{n=1}^N \left( \sum \limits_{k=1}^k t_{nk}ln(y_{nk})\right)$$
$$ \nabla L_{\bm{w_j}} (\bm{w_1,...,w_k})= \sum \limits_{n=1} ^{N} (y_{nj} - t_{nj})\phi_n$$
\end{itemize}



\section{Bias-Variance trade-off and model selection}
\subsection{Free Lunch Theorem}
\begin{itemize}
\item Generalization accuracy (= on test set) of learner L =
$ Acc_G(L)$
\item All possible concepts $\mathcal{F}$
\item True models to be learnt $y=f(x)$
\item \textbf{No-Free Lunch Theorem}
$$ \text{For any learner L: } \frac{1}{|\mathcal{F}|} \sum \limits_{\mathcal{F}}Acc_G(L) = \frac{1}{2}$$
\item For learner $L_1, L_2$ 
$$ \text{if } \exists \text{ learning problem so that } Acc_G(L_1)>Acc_G(L_2) $$
$$ \text{then }\exists \text{ learning problem } Acc_G(L_2)>Acc_G(L_1)$$
\end{itemize}

\subsection{Bias-Variance}
\begin{itemize}
\item Dataset D with N samples obtained by function $t_i = f(x_i)+\epsilon$
$$ E[\epsilon]=0$$
$$ Var[\epsilon] = \sigma^2$$
\item Aim is to find model $y(x)$  that approximates f as well as possible. Expected square error on unseen samples x
\begin{align*}
 E[(t-y(x))^2] &= E[t^2+y(x)^2 -2ty(x)]=E[t^2]+E[y(x)^2]-2E[ty(x)]  \\
 &= E[t^2] \pm E[t]^2 + E[y(x)^2] \pm E[y(x)]^2 -2tE[y(x)] \\
 &= Var[t]+E[t]^2+Var[y(x)]+E[y(x)]^2-2tE[y(x)] \\
 &= Var[t]+Var[y(x)]+ (E[t]^2 + E[y(x)]^2 - 2tE[y(x)])\\
 &= \underbrace{Var[t]}_{\sigma^2}+\underbrace{Var[y(x)]}_{Variance}+\underbrace{E[(t-y(x)]^2}_{Bias^2}
\end{align*}

\item \textbf{Bias} is difference between truth and what is expected to be learnt 
$$ bias^2= \int (f(\bm{x})-E[y(\bm{x})])^2p(\bm{x})d\bm{x}$$
\begin{itemize}
\item Decreases with \textbf{more complex models}
\end{itemize}

\item \textbf{Variance} is difference between what is learnt from a particular dataset and what is expected to be learnt 
$$ \int E[(y(\bm{x}) - \bar{y}(\bm{x}))^2]p(\bm{x})d\bm{x}$$
$$ \bar{y}(\bm{x})=E[y(\bar{x})]$$
\begin{itemize}
\item Decreases with \textbf{simpler models}
\item Decreases with \textbf{more samples}
\end{itemize}

\item Bias-Variance can be calculated analytically in KNN
$$ E[(t-y(x))^2]=  \sigma^2 + \frac{\sigma^2}{K}+\left(  f(\bm{x}) -\frac{1}{K} \sum \limits_{i=1}^Kf(\bm{x}_i) \right)^2$$

\subsection{Train-Test Errors}
\item \textbf{Training error} $\rightarrow$ optimistically biased estimate of prediction
\begin{itemize}
\item Regression $ L_{train} = \frac{1}{N} \sum \limits_{n=1}^N (t_n - y(\bm{x}_n))^2$
\item Classification $ L_{train} = \frac{1}{N} \sum \limits_{n=1}^N (I(t_n \neq y(\bm{x}_n)))$
\end{itemize}

\item \textbf{Prediction error} 
\begin{itemize}
\item Regression : $L_{true} = \int (f(\bm{x}) - y(\bm{x}))^2p(\bm{x})d\bm{x}$
\item Classification : $L_{true} = \int I(f(\bm{x}) \neq y(\bm{x}))p(\bm{x})d\bm{x}$
\end{itemize}


\item \textbf{Test error} $\rightarrow$ unbiased if not used during training
 $$ L_{train} = \frac{1}{N_{test}} \sum \limits_{n=1}^{N_{test}} (t_n - y(\bm{x}_n))^2$$
Different estimation methods for test error (never use \textbf{test data!}):\\
Direct approach
\begin{itemize}
\item \textbf{Leave-One-Out Cross Validation} : $D-\{n\}$ n-th data point moved to validation set (almost unbiased, slightly pessimistic)
$$ L_{LOO} = \frac{1}{N} \sum \limits_{n=1}^N (t_n - y_{D-\{n\}}(\bm{x}_n))^2 $$

\item \textbf{K-Fold Cross Validation} : divide training data into k equal parts $D_1 ,...,D_k$ and train on $D-D_i$ (faster but more pessimistically biased)
$$ L_{D_i} = \frac{k}{N} \sum \limits_{(x_n,t_n) \in D_i}(t_n - y_{D-D_i}(\bm{x}))^2$$
$$ L_{k-fold} = \frac{1}{k} \sum \limits_{i=1}^k L_{D_i}$$

Adjustment techniques on training error to take into account model complexity:
\item $\bm{C_p}$ : d total number of parameters, $(\tilde\sigma )^2$ estimate of variance of $\epsilon$
$$ C_p = \frac{1}{N}(RSS + 2d\tilde{\sigma}^2)$$
\item \textbf{AIC} : L maximized value for likelihood of model 
$$ AIC = -2logL+2d$$
\item \textbf{BIC} : since $logN > 2$ for any $n>7$ , BIC selects smaller models
$$ BIC = \frac{1}{N}(RSS+ log(N)d \tilde{\sigma}^2)$$

\item \textbf{Adjusted R2} : large values indicate small test error
$$ AdjR^2 = 1 - \frac{RSS (N-1)}{TSS(N-d-1)}$$
\end{itemize}

\end{itemize}


\subsection{Model selection}
\begin{itemize}
\item \textbf{Feature selection} 
\begin{enumerate}
\item $M_0$ null model ,no features,predicts mean
\item $k=1,...M$ fit all $\binom{M}{k}$ model with k features
\item Pick best among these $\binom{M}{k}$ called $M_k$
\item Selected single best with CV,AIC,BIC...
\end{enumerate}
Bad if M too large (overfitting + computational cost) 
\begin{itemize}
\item Filter method = \textbf{PCA}, \textbf{SVD}...
\item Embedded = \textbf{Decision Tree,Lasso}...
\item Wrapper  = GA + Algorithm to find weights with \textbf{forward} or \textbf{backward} step-wise selection
\end{itemize}

\item \textbf{Regularization} : Ridge ,Lasso
\item \textbf{Dimension Reduction} : unsupervised, transforms original features.\\
\textbf{PCA} : project on the input subspace that account for most of the variance (repeat for m lines)
\begin{enumerate}
\item Mean center data : $\bar{\bm{x}} = \frac{1}{N} \sum \bm{x}_n$
\item Compute covariance matrix \textbf{S}
\item Compute eigenvalues/eigenvectors  of \textbf{S} : $$S= \frac{1}{N-1}\sum_{n=1}^N (\bm{x_n-\bar{x}}) (\bm{x_n-\bar{x}})^T$$
\item Eigenvector $e_1$ with largest eigenvalue $\lambda_1$ is first principal component , $\frac{\lambda_k}{\sum \limits_i \lambda_i}$ is portion of \textbf{explained variance}
\end{enumerate}
\end{itemize}

\subsection{Model Ensembles}
\begin{itemize}
\item \textbf{Bagging} : reduces variance,without increasing bias
$$ Var(\bar{x}) =\frac{Var(x)}{N} $$
Multiple models $\rightarrow$ More training sets : \textbf{Bootstrap aggregation}, random sampling with replacement. Only good with \textbf{unstable learners} (bad when there is high bias).
\item \textbf{Boosting} : sequentially learn weak classifiers  ( start from equal weights then penalize samples with misspredictions)
\end{itemize}


\section{PAC-Learning and VC Dimensions}
\subsection{Intro and Version Space}
\begin{itemize}
\item Set of instances X
\item Set of hypothesis H (finite)
\item Set of possible target concepts C
\item Training instances generated by a fixed,unknown probability distribution P over \textbf{X}.



\begin{center}
\fbox{\begin{minipage}{30em}
The learner observers a sequence D of training samples $ \langle x,c(x) \rangle $  for some target concept $c \in C$ and it must output a hypothesis h estimating c,  which is evaluated by its performance on subsequent instances drawn from P 
$$ L_{true} =Pr_{x \in P}[c(x) \neq h(x)]$$
\end{minipage}}
\end{center}

\item $L_{true}$ cannot be measured as P is unknown so aim is to \textbf{bound} $L_{train}$ given $L_{true}$
\item Given a \textbf{Version Space} ( subset of hypotheses in H consistent with training data : $L_{train}=0$) , can $L_{true}$ also be bounded to be in the VS?


\begin{center}
\fbox{\begin{minipage}{30em}
If the hypothesis space H is finite and D is a sequence of N $\geq 1$  independent random samples of some target concept c, the for any  $0 \leq \epsilon \leq 1$,  the probability that  $VS_{H,D}$ ,contains an hypothesis error greater than  $\epsilon$  is less than  $|H|e^{-\epsilon N}$
$$ Pr(\exists h \in H : L_{train} =0 \land L_{true} \geq \epsilon) \leq |H|e^{-\epsilon N}$$
\end{minipage}}
\end{center}
Proof:
\begin{align*}
 Pr((&L_{train}(h_1) = 0 \land L_{true}(h_1) \geq \epsilon) \lor
 ... \lor (L_{train}(h_{|H|})= 0 \land L_{true}(h_{|H|}) \geq \epsilon))\\
  &\leq \sum \limits_{h \in H} Pr(L_{train}(h)= 0 \land L_{true}(h) \geq \epsilon)\\
&\leq \sum \limits_{h \in H} Pr(L_{train}(h)= 0 | L_{true}(h) \geq \epsilon)\\
&\leq \sum \limits_{h \in H} (1-\epsilon)^N\\
&\leq |H|(1-\epsilon)^N \\
&\leq |H|e^{-\epsilon N}
\end{align*}

\item That probability should be at most $\delta$
$$ |H|e^{-\epsilon N} \leq \delta$$
\item  N can be computed as
$$ N \geq \frac{1}{\epsilon}(ln |H| + ln(\frac{1}{\delta})$$
\item $\epsilon$ can be computed as
$$ \epsilon \geq \frac{1}{N} (ln |H| + ln(\frac{1}{\delta})$$
N.B. $ln |H| $ is still exponential as $|H| = 2^{2^M}$

\subsection{PAC-Learning}
\newpage
\item \textbf{PAC Learning}
\begin{center}
\fbox{\begin{minipage}{30em}
Given a class C of possible target concepts defined over a set of instances X of length n , and a Learner L using hypothesis space H. C is \textbf{PAC- LEARNABLE} if there exists an algorithm L such that for every $f \in C$ for any distribution P , for any $\epsilon$ such that $0 \leq \epsilon \leq \frac{1}{2}$ and $\delta$ such that $0 \leq \delta \leq \frac{1}{2}$ , algorithm L with probability at least $1-\delta$ outputs a concept h such that $L_{true} \leq \epsilon$ using a number of samples that is polynomial of $\frac{1}{\epsilon} $ and $\frac{1}{\delta}$ 
\end{minipage}}
\end{center}

\item \textbf{Efficient PAC-Learnable} 
\begin{center}
\fbox{\begin{minipage}{30em}
 C is \textbf{efficiently PAC- LEARNABLE} if there exists an algorithm L such that for every $f \in C$ for any distribution P , for any $\epsilon$ such that $0 \leq \epsilon \leq \frac{1}{2}$ and $\delta$ such that $0 \leq \delta \leq \frac{1}{2}$ , algorithm L with probability at least $1-\delta$ outputs a concept h such that $L_{true} \leq \epsilon$ using a number of samples that is polynomial of $\frac{1}{\epsilon} $ and $\frac{1}{\delta}$ , M and size(c). 
\end{minipage}}
\end{center}

\item \textbf{Agnostic Learning} : empty VS ( no $L_{train} = 0$ )
$$ L_{true} (h)  \leq L_{train} + \epsilon$$
$$ \text{Hoeffding bound: } Pr(E[\bar{X}] - \bar{X} > \epsilon) < e^{-2n\epsilon^2}$$
\begin{center}
\fbox{\begin{minipage}{30em}
Hypothesis space H finite, dataset D with i.i.d samples ,$0 \leq \epsilon \leq 1$ , for any learned hypothesis h :
$$ Pr(L_{true}(h)-L_{train}(h) > \epsilon ) \leq |H|e^{-2N\epsilon^2}$$
\end{minipage}}
\end{center}

\item \textbf{PAC-Bound vs B/V Trade-off}
$$ L_{true}(h) \leq \underbrace{L_{train}(h)}_{Bias} + \underbrace{\sqrt{\frac{ln|H|+ ln \frac{1}{\delta}}{2N}}}_{Variance}$$
Large $|H|$: \textbf{low bias} , \textbf{high variance}\\
Small $|H| $: \textbf{high bias} , \textbf{low variance} (tighter bound)
\end{itemize}

\subsection{VC Dimension}
\begin{itemize}
\item For $|H| \rightarrow \infty$ PAC-Learning bound cannot be used but number of points N required can be found using an alternative method ( "Axis -aligned rectangles" ) $$ N \geq \left( \frac{4}{\epsilon} \right) ln \left( \frac{4}{\delta} \right)$$
The best way for $|H| \rightarrow \infty$ is to bound the error as a function of the number of points that can be completely labeled

\item \textbf{Dichotomy}
\begin{center}
\fbox{\begin{minipage}{30em}
A \textbf{dichotomy} of a set S is a partition of S into two disjoint subsets.
\end{minipage}}
\end{center}

\item \textbf{Shattering}
\begin{center}
\fbox{\begin{minipage}{30em}
A set of instances S is \textbf{shattered} by a hypothesis space H if and only if for every dichotomy of S there exists some hypothesis in H \textbf{consistent} (= classify correctly) with this dichotomy.
\end{minipage}}
\end{center}

\item \textbf{VC-Dimension}
\begin{center}
\fbox{\begin{minipage}{30em}
The VC dimension, VC(H) , of an hypothesis space H defined over instance space X is the size of the \textbf{largest finite subset} shattered by H. If arbitrarily large finite sets of X can be shattered by H , then VC(H) = $\infty$  
\end{minipage}}\\
Rule of thumb : number of parameters = max number of points
\end{center}

\item Number of samples to guarantee an error of at most $\epsilon$ with probability at least $(1-\delta)$
$$ N \geq \frac{1}{\epsilon} \left( 4log_2\left( \frac{2}{\delta}\right) + 8VC(H)log_2\left( \frac{13}{\epsilon}\right)\right)$$
\item PAC Bound and VC Dimension
$$ L_{true} \leq L_{train} + \sqrt{\frac{VC(H)\left( ln \frac{2N}{VC(H)} + 1 \right)+ ln \frac{4}{\delta}}{N}} $$
Choose hypothesis space H so to minimize the bound on expected true error

\item VC-Dimension properties
\begin{center}
\fbox{\begin{minipage}{30em}
The VC Dimension of a space $|H| \leq \infty$ is bounded from above
$$ VC(H) \leq log_2(|H|)$$
If $|H|=d$ then there are at least $2^d$ functions in H. Since there are at least $2^d$ labelings : $|H| \geq 2^d$
\end{minipage}}
\end{center}
\begin{center}
\fbox{\begin{minipage}{30em}
A concept class C with $VC(C) = \infty $ is \textbf{not} PAC -Learnable
\end{minipage}}
\end{center}
\end{itemize}

\section{Kernel Methods}
\begin{itemize}
\item Make linear models work in non-linear settings by mapping data into higher dimensions where it exhibits linear patterns. Mapping changes feature representation, which can be expensive but made "affordable" with the \textbf{kernel trick}.Can be used if algorithm used scalar product than can be replaced by kernel function.
$$ \text{Mapping } \phi : \bm{x} \rightarrow \{ x^2_1, x_m^2,x_1x_2,...,x_1x_M,...,x_{M-1}x_M\}$$ 
$$ \text{Inefficient and many features (can blow up)}$$
$$\text{Solution : dual problem with kernels}$$
$$ k(x,x') = \phi(\bm{x})^T \phi(\bm{x'})$$
$$ \phi \text{ fixed non-linear feature space mapping (\textbf{basis function})}$$

\item $k(\bm{x},\bm{x'})$ = similarity between \textbf{x ,x'}

\item Simplest kernel = linear kernel 
$$ k(\bm{x,x'}) = \bm{x^Tx'}$$

\item Kernel is symmetric in its arguments
$$ k(\bm{x,x'})=k(\bm{x',x})$$

\item \textbf{Stationary kernels}
$$ k(\bm{x,x'}) = k(\bm{x-x'})$$

\item \textbf{Homogeneous kernels}
$$ k(\bm{x,x'}) = k(||\bm{x-x'}||)$$

\subsection{Dual representation}
\begin{itemize}
\item Linear regression model with L2 regularized RSS 
$$ L(\bm{w}) = \frac{1}{2}\sum \limits_{n=1}^{N} (\bm{w}^T \phi(x_n)-t_n)^2 + \frac{\lambda}{2}\bm{w^Tw}$$
$$ \bm{w} = -\frac{1}{\lambda} \sum \limits_{n=1}^N (\bm{w^T}\phi(\bm{x_n} -t_n)\phi(\bm{x}_n) = \sum \limits_{n=1}^N a_n \phi(\bm{x_n})= \bm{\Phi^T a} $$

\item Design matrix $\bm{\Phi}$ has $n^{th}$ row = $\phi(\bm{x_n})^T$
\item Coefficients $\bm{a_n}$ are functions of \textbf{w} = $-\frac{1}{\lambda}(\bm{w^T\phi(x_n)} -t_n)$
\item \textbf{Gram matrix} $\bm{K = \Phi^T \Phi}$ , $N x N$ matrix:
$$ K_{nm} = \phi(x_n)^T \phi(x_m)= k(\bm{x_n,x_m})$$
$$ K =\begin{bmatrix}
k(\bm{x_1,x_1}) & ... & k(\bm{x_1,x_N}) \\
... & ... &...\\
k(\bm{x_N,x_1}) & ... & k(\bm{x_N,x_N})
\end{bmatrix} $$

\item Error function in terms of Gram Matrix of Kernel 
$$ L(\bm{w}) = \frac{1}{2} \bm{a^T \Phi\Phi^T\Phi\Phi^Ta - a^T\Phi\Phi^Tt}+ \frac{1}{2} \bm{t^Tt} + \frac{\lambda}{2}\bm{a^T\Phi\Phi^Ta}$$
$$ L_a = \frac{1}{2} \bm{a^T}KK\bm{a} - \bm{a}^TK \bm{t} + \frac{1}{2}\bm{t^Tt} + \frac{\lambda}{2} \bm{a^T}K\bm{a}$$
Solving for a using $\bm{w = \Phi^Ta}$ and $a = -\frac{1}{\lambda}(\bm{w^T}\phi(x_n)-t_n)$
$$ \bm{a} = (K+\lambda \bm{I}_N)^{-1}\bm{t}$$
Solution for \textbf{a} can be expressed as linear combination of elements of $\phi(x)$ whose coefficients depends entirely in terms of $k(x,x')$ from which the original formulation of w can be recovered.

\item \textbf{Prediction} for new input \textbf{x}
$$ y(\bm{x}) = \bm{w}^T\phi(\bm{x}) = \bm{a}^T\Phi \phi(\bm{x})= \bm{k}(\bm{x})^T(K+ \lambda \bm{I}_N)^{-1}\bm{t}$$
$$ \text{where } \bm{k(x)} = k(\bm{x_n,x}) $$
\end{itemize}

\item Advantages
\begin{itemize}
\item avoid working with feature vector $\phi(x)$
\item build a feature space with high dimensionality
\item leverage possibility to use not only vectors of real numbers but also over objects (sets,logic formulas...)
\end{itemize}

\item Disadvantage
\begin{itemize}
\item Invert big $N x N$ matrix instead of (possibly) smaller $M x M$ matrix
\end{itemize}
\end{itemize}

\subsection{Constructing Kernels}
\begin{itemize}
\item 1. Method :Choose feature space mapping $\phi(x)$ and use it to find kernel.Example with 1-D input space:
$$ k(\bm{x,x'})=\phi(\bm{x})^T \phi(\bm{x'}) = \sum \limits_{i=1}^N \phi_i(\bm{x})\phi_i(\bm{x'})$$

\item 2.Method : Construct kernels directly , making sure it is valid ( = corresponds to some \textbf{scalar product} in some feature space). Example $k(x,z) =(x^Tz)^2$ in 2-D space: 
\begin{align*}
 k(x,z)=(x^Tz)^2 &=(x_1z_1+x_2z_2)^2=x_1^2z_1^2+2x_1z_1x_2z_2+x_2^2z_2^2\\
 &=(x^2_1,\sqrt{2}x_1x_2,x^2_2)(z_1^2,\sqrt{2}z_1z_2,z^2_2)^T=\phi(\bm{x})^T\phi(\bm{z})
\end{align*}

\item Necessary and sufficient condition for a function $k(\bm{x,x'})$ to be a kernel is that the Gram matrix K is \textbf{semi-definite} for all possible choices of the set $\{ x_n \}$:
$$ \bm{x}^TK\bm{x} \geq 0 \text{ for non-zero vectors } \bm{x}$$
$$ \sum \limits_n \sum \limits_m K_{n,m}\bm{x_n}\bm{x_m}$$
\begin{center}
\fbox{\begin{minipage}{30em}
Mercer's theorem : Any continuous, symmetric,positive semi-definite (= no negative eigenvalues) kernel function K(x,y) can be expressed as a \textbf{dot product} in a high-dimensional space
\end{minipage}}
\end{center}
\end{itemize}

\subsection{List of valid kernels}
Given kernels $k_1(\bm{x,x'}), k_2{\bm{x,x'}}$
\begin{enumerate}
\item $k(\bm{x,x'}) = ck_1(\bm{x,x'})$
\item $k(\bm{x,x'}) = f(\bm{x})k_1(\bm{x,x'})f(\bm{x})$
\item $k(\bm{x,x'}) = q(k_1(\bm{x,x'})), \text{ q() polynomial with non negative coefficients}$
\item $k(\bm{x,x'}) = exp(k_1(\bm{x,x'}))$
\item $k(\bm{x,x'}) = k_1(\bm{x,x'}) +  k_2(\bm{x,x'})$
\item $k(\bm{x,x'}) = k_1(\bm{x,x'}) \cdot k_2(\bm{x,x'})$
\item $k(\bm{x,x'}) = k_3(\bm{\phi(x),\phi(x')})$
\item $k(\bm{x,x'}) = \bm{x}^TA\bm{x'} \text{ A symmetric positive semi-definite matrix}$
\item $k(\bm{x,x'}) = k_a(\bm{x_a,x_a'}) +  k_b(\bm{x_b,x_b'}) \text{ where xa,xb are variables with x=(xa,xb)}$
\item $k(\bm{x,x'}) = k_a(\bm{x_a,x_a'}) \cdot k_b(\bm{x_b,x_b'}) $
\end{enumerate}

\begin{itemize}
\item Common kernel : \textbf{Gaussian}
$$ k(\bm{x,x'}) = e^{-\frac{||\bm{x-x'}||^2}{2\sigma^2}}$$
$$ \text{Valid : } ||x-x'||^2= (x-x')^T(x-x')=x^T + x'^Tx' - 2x^Tx'$$
$$ k(x,x') = e^{-\frac{x^Tx}{2\sigma^2}}e^{-\frac{x^Tx'}{\sigma^2}}e^{-\frac{x'^Tx'}{2\sigma^2}}$$
$$ \text{Valid by combining rules 2 and 4 + linear kernel}$$
\end{itemize}


\section{SVM}
SVM use :
\begin{itemize}
\item Subset of training samples (\textbf{support vectors})
\item Vector of weights \textbf{a}
\item Similarity function $K(x,x')$ (\textbf{kernel})
\end{itemize}

\begin{itemize}
\item Class prediction for new sample $x_q (t_i \in \{-1,1 \})$
$$ f(x_q) = sign \left( \sum \limits_{m \in S} \alpha_mt_mk(x_q,x_m) + b \right)$$
where \begin{itemize}
\item S is the set of indices of \textbf{support vectors}
\end{itemize}

\item Can be seen as generalization of perceptron $f(x_q) =sign \left( \sum \limits_{j=1}^M w_j\phi_j(x_q) \right)$
Where 
$$ w_j = \sum \limits_{n=1}^N \alpha_nt_n\phi_j(x_n)$$
$$ f(x_q) =sign \left( \sum \limits_{j=1}^M \left( \sum \limits_{n=1}^N \alpha_nt_n\phi_j(x_n)\right) \phi_j(x_q) \right) $$
$$ f(x_q) =  \sum \limits_{n=1}^N \alpha_nt_n \left( \phi(x_n) \phi(x_q) \right)$$

\item Get a much powerful learner by replacing dot product with \textbf{similarity function} ( kernel matrix)
\end{itemize}


\subsection{Learning in SVM}
\begin{itemize}
\item \textbf{Margin }: smallest distance between separating hyperplane and any of the samples. Aim is to \textbf{maximize margin} = maximize distance of the closest points.

\item Assuming a separating hyperplane exists:
$$ y(x_n) = \bm{w}^T\bm{\phi(x_n)}+b $$
$$ t_ny(x_n) > 0 \quad \forall n$$
$$ \text{Distance of point to surface} : \frac{t_ny(x_n)}{||\bm{w}||}= \frac{t_n(\bm{w}^T\phi(x_n)+b)}{||\bm{w}||}$$ 

\item \textbf{Weigh optimization problem} 
$$ \text{margin} = min_n t_n(\bm{w}^T\phi(\bm{x_n})+b)$$
$$ \bm{w^*} = argmax_{\bm{w},b}  \left( \frac{1}{||w||_2} min_n(t_n(\bm{w}^T\phi(x_n)+b) \right)$$

\item Problem hard to solve. So fix margin (it's \textbf{scale invariant}) to be 1:
$$ t_n(\bm{w}^T\phi(\bm{x_n})+b)=1$$
\item Now \textbf{minimize weights}
\begin{itemize}
\item \textbf{Minimize}: $\frac{1}{2}||\bm{w}||_2^2 $ (Equivalent to maximize $\frac{1}{||\bm{w}||}$)
\item \textbf{Subject to}: $t_n(\bm{w}^T\phi(\bm{x_n})+b) \geq 1 ,\quad \forall n$
\end{itemize}
\end{itemize}

\subsubsection{Constraint optimization recap}
\begin{itemize}
\item \textbf{Minimize} $f(\bm{w}) \rightarrow$ quadratic
\item \textbf{Subject to} $h_i(\bm{w})=0, for=1,2,...\rightarrow$ quadratic
\item $\bm{w^*}, \nabla f(\bm{w^*})$ solution must lie in subspace spanned by $$ \{\nabla h_i(\bm{w^*}:i=1,2...) \}$$

\item \textbf{Lagrangian function}
$$ L(\bm{w},\lambda)= f(\bm{w})+\sum \limits_i \lambda_ih_i(\bm{w})$$

\item \textbf{Lagrangian multipliers} $\lambda_i$ , to solve 
$$ \nabla L (\bm{w^*}, \bm{\lambda^*})=0$$

\item With \textbf{inequality constraints}
\begin{itemize}
\item \textbf{Minimze} $f(\bm{w})$
\item \textbf{Subject to} $g_i(\bm{w}) \leq 0 \quad i=1,2...$
\item \textbf{Subject to} $h_i(\bm{w}) = 0 \quad i=1,2..$
\end{itemize}

\item Lagrange multipliers for \textbf{equality} are $\lambda_i$ and for \textbf{inequality} are $\alpha_i$

\item \textbf{KKT} conditions
$$\lambda(\bm{w^*,\alpha^* , \lambda^*})=0$$
$$ h_i(\bm{w^*})=0$$
$$ g_i(\bm{w^*}) \leq 0$$
$$ \alpha^*_i \geq 0 $$
$$ \alpha^*_ig_i(\bm{w})=0$$

\item Constraints are either active $g_i(\bm{w^*})=0$ (\textbf{IT'S A SUPPORT VECTOR}) or its multiplier is zero $\alpha^*_i=0$
\end{itemize}

\subsubsection{Dual and Primal Problem}
\begin{itemize}
\item Primal : weights over \textbf{features}
\item Dual : weights over \textbf{instances} $\rightarrow $ easier and has more weights=0

\item \textbf{Lagrangian function}
$$ L(\bm{w},b,\bm{\alpha})= \frac{1}{2}||\bm{w}||_2^2 - \sum \limits_{n=1}^N \alpha_n(t_n(\bm{w}^T\phi(\bm{x_n})+b)-1)$$

\item Gradient for w,b:
$$ \bm{w}= \sum \limits_{n=1}^N \alpha_nt_n\phi(x_n)$$
$$ 0 = \sum \limits_{n=1}^N \alpha_nt_n$$

\item Rewrite Lagrangian
\begin{itemize}
\item \textbf{Maximize: } $\tilde{L}(\bm{\alpha})= \sum \limits_{n=1}^N \alpha_n - \frac{1}{2}\sum \limits_{n=1}^N \sum \limits_{m=1}^N \alpha_n\alpha_mt_nt_mk(\bm{x_n,x_m})$
\item \textbf{Subject to:} $\alpha_n \geq 0 \quad \forall n$
\item \textbf{Subject to:} $\sum \limits_{n=1}^N \alpha_nt_n=0$
\end{itemize}
\end{itemize}

\subsection{Prediction}
$$ y(x) = sign\left(  \sum \limits_{n=1}^N \alpha_nt_nk(x,x_n)+b \right)$$
$$ b= \frac{1}{N_S}\sum \limits_{n \in S} \left( t_n -\sum \limits_{m \in S} \alpha_m t_m k(\bm{x_n,x_m}) \right)$$
\begin{itemize}
\item As number of dimension increases also the number of \textbf{support vectors} increases. Ideally $N_S << N$ , otherwise overfit
\end{itemize}

\subsection{Solution Techniques}
\begin{itemize}
\item \textbf{Sequential minimal optimization} :
\begin{enumerate}
\item Find sample $x_i$ that violates KKT
\item Select second sample heuristically
\item Joint optimize $\alpha_i,\alpha_j$
\end{enumerate}
\end{itemize}

\subsection{Noisy data and Slack Variables}
With noisy data missclassification should be allowed using a \textbf{relaxation} of the problem.
\begin{itemize}
\item Slack variable $\xi_i$ that allow to violate the constraints, but a \textbf{penalty cost} C
\begin{itemize}
\item \textbf{Minimize: } $||w||^2_2 + C \sum \limits_i \xi_i$
\item \textbf{Subject to: } $t_i (\bm{w}^Tx_i + b) \geq 1-\xi_i$
\item \textbf{Subject to: } $\xi_i> 0$
\end{itemize}

\item \textbf{C} allows to trade-off \textbf{bias} and \textbf{variance} (CV to find right C)
\begin{itemize}
\item \textbf{High C} : complex solution
\item \textbf{Low C} : simpler solution
\end{itemize}

\item Dual with Slack
\begin{itemize}
\item \textbf{Maximize: } $\tilde{L}(\bm{\alpha})= \sum \limits_{n=1}^N \alpha_n - \frac{1}{2}\sum \limits_{n=1}^N \sum \limits_{m=1}^N \alpha_n\alpha_mt_nt_mk(\bm{x_n,x_m})$
\item \textbf{Subject to:} $0 \leq \alpha_n \leq C \quad \forall n$
\item \textbf{Subject to:} $\sum \limits_{n=1}^N \alpha_nt_n=0$
\item \textbf{Support vectors} : $\alpha_n > 0$
\item \textbf{Point on margin}: $\alpha_n < C$
\item \textbf{Lies inside margin} : $\alpha_n = C$ , is correctly classified $\xi_i \leq 1$ or \textbf{missclassified} if $\xi_i >1$
\end{itemize}
\end{itemize}

\subsection{Bounds}
\begin{itemize}
\item Bound on VC dimension \textbf{decreases with margin} : \textbf{large margin} = \textbf{low VC Dimension} = \textbf{low variance}

\item Margin bound is very pessimistic
\item Use \textbf{Leave-one-out Bound}, can be easily computed
$$ L_h \leq \frac{E[\text{Number of support vectors}]}{N}$$ 
Remove 1 samples at the time, if you remove a non-support vector , margin does not change. If you remove support vector margin changes!
\end{itemize}


\section{Reinforcement Learning}
\begin{itemize}
\item \textbf{Agent} : learns and makes decision, acting and observing the environment. At each step t ,the agent can:
\begin{itemize}
\item Perform action $a_t$
\item Receive observation $o_t$
\item Receive scalar reward $r_t$
\end{itemize}
\item \textbf{The environment} : interacts with the agent (anything that the agent can control). At each step t,it can :
\begin{itemize}
\item Receive action $a_t$
\item Emit observation $o_t$
\item Emit scalar reward $r_t$
\end{itemize}
\item The \textbf{History} is a sequence of actions,observations and rewards: 
$$ h_t= a_1,o_1,r_1, ... ,a_t,o_t,r_t$$
\begin{itemize}
\item It influences the next action to be chosen by the agent and which observation /reward the environment will emit.
\end{itemize}
\item The \textbf{state} is the information used to determine what happens next . It is a function of the history 
$$ s_t^a = f(a_1,o_1,r_1, ... ,a_t,o_t,r_t)$$
\item The state of the environment is the private internal representation of the environment. It is usually \textbf{not visible} to the agent ( it is in some card games for example)

\item If environment is \textbf{fully observable}
$$ o_t=s_t^a=s_t^e$$

\item \textbf{Reinforcement Learning} is useful when:
\begin{itemize}
\item Dynamics of environment  are \textbf{unknown} or \textbf{difficult to model}
\item The model of the environment is \textbf{too complex } to be solved exactly, so approximate solutions are needed.
\end{itemize}
\end{itemize}

\subsection{Markov Decision Process}
"Future is independent on the past given the present"

\begin{itemize}
\item A \textbf{stochastic process} $X_t$ is \textbf{Markovian} if and only if
$$ P(X_{t+1}=j | X_t =i , X_{t-1}= k_{t-1},...,X_1=k_1,X_0=k_0) = P(X_{t+1}= j|X_t=i)$$

\item Means that the \textbf{current} state is a sufficient statistic to calculate the probability of the next value ( no past is needed).

\item If transition probabilities are  \textbf{time invariant} 
$$ p_{ij} = P(X_{t+1}=j|X_t =i )=P(X_t= j|X_0=i)$$

\end{itemize}
\textbf{Markov Decision Process}
\begin{itemize}
\item An MDP is Markov reward process with \textbf{decisions}.It models an environment in which all states are	Markov and time is divided into \textbf{stages}.
$$ \langle S,A,P,R,\gamma,\mu \rangle$$
\begin{itemize}
\item \textbf{S} : set of states (finite)
\item \textbf{A} : set of actions (finite). Can be function of state
\item \textbf{P} : state-transition probability matrix $P(s'|s,a)$ of size $(|S|+|A|)x|S|$ $P(s'|s,a)$
\item \textbf{R} : reward function $R(s,a)=E[r|s,a]$
\item $\bm{\gamma}$: discount factor $\in [0,1]$\\
How much the reward will lose in 1 time-step
\begin{itemize}
\item $\gamma \rightarrow 0 $ \textbf{myopic evaluation}
\item $\gamma \rightarrow 1 $ \textbf{far-sighted evaluation}
\end{itemize}
\item $\bm{\mu}$: set of initial probabilities $\mu_i^0=P(X_0=i)$
\end{itemize}
\end{itemize}

\subsubsection{Reward and Goals}
\begin{itemize}
\item \textbf{Sutton Hypothesis}: goals and purposes can be thought of as the \textbf{maximization} of the cumulative sum of a received scalar reward
\item Goal can be defined by infinite \textbf{different reward functions} :but it must always be outside the agent's control which can simply measure success step by step (\textbf{explicitly} and \textbf{frequently}).

\item Time horizons can be:
\begin{itemize}
\item \textbf{finite}: horizon reduces at each step so at every time there is a different optimization problem
\item \textbf{indefinite}: until some stopping criteria is met , like \textbf{absorbing states} (e.g.: Blackjack)
\item \textbf{infinite} : ideally infinite (e.g.: Pole balancing)
\end{itemize}

\item \textbf{Cumulative rewards} can be:
\begin{itemize}
\item \textbf{total reward} : $V = \sum \limits_{i=1}^{\infty}r_i$
\item \textbf{average reward}: $V= \lim_{n \to \infty}\frac{r_1+...+r_n}{n}$
\item \textbf{discounted reward:} $V= \sum_{i=1}^{\infty} \gamma^{i-1}r_i$ 
\end{itemize}
\end{itemize}

\textbf{Infinite time-horizon discounted return}
\begin{itemize}
\item $v_t = r_{t+1} + \gamma r_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} $ 
\end{itemize}
\subsubsection{Policies}
\textbf{Policies}
\begin{itemize}
\item \textbf{Policy} : mapping from states to probabilities of selecting each possible action. Decides which action an agent selects.

\item Policies can be :
\begin{itemize}
\item \textbf{Markovian} $\subset$ History-dependent
\item \textbf{Deterministic} $\subset$ Stochastic
\item \textbf{Stationary} $\subset$ Non-stationary
\end{itemize}

\item Policy $\pi$ is a distribution over actions given a state
$$ \pi(a|s) = P[a|s]$$
\item MDP policies depend on the current state (stationary property).\\
Given an MDP $M$ and a policy $\pi$
\begin{itemize}
\item State sequence $s_1,s_2...$ is a \textbf{Markov} process $\langle S, P^{\pi},\mu \rangle$
\item The state and reward sequence $s_1,r_2,s_2,...$ is a \textbf{Markov	reward process} ( Markov Chain ) $\langle S^{\pi} ,P^{\pi} , R^{\pi}, \gamma ,\mu \rangle$
$$ P^{\pi} = \sum_{a \in A} \pi(a|s)P(s'|s,a)$$
$$ R^{\pi} = \sum_{a \in A } \pi(a|s)P(s'|s,a)$$
\end{itemize}
\end{itemize}


\subsubsection{Value Function}
\begin{itemize}
\item Given a policy $\pi$ it is possible to define the utility of \textbf{each state} using \textbf{Policy Evaluation}

\item The \textbf{state-value function } $V^{\pi}(s)$ of an MDP is the expected return starting from state s and then following policy $\pi$
$$ V^{\pi} = E_{\pi} [v_t|s_t=s]$$

\item For control purposes it is better to define the value of each action in each state using\textbf{action-value function } $Q^{\pi}(s,a)$ 
$$ Q^{\pi}(s,a)= E_{\pi}[v_t | s_t = s,a_t=a]$$

\subsubsection{Bellman Equations}
\item \textbf{Bellman equation} for decomposed state-value function  (immediate reward + discounted value of successor state )
\begin{align*}
 V^{\pi}(s) &= E_{\pi}[r_{t+1}+ \gamma V^{\pi}(s_{t+1}| s_t = s]\\
 &= \sum_{a \in A} \pi(a|s) \left( R(s,a)+ \gamma \sum_{s' \in S}P(s'|s,a)V^{\pi}(s') \right)
\end{align*}

\item \textbf{Bellman equation} for decomposed state-action function
\begin{align*}
 Q^{\pi}(s,a) &= E_{\pi}[r_{t+1}+ \gamma Q^{\pi}(s_{t+1},a_{t+1}| s_t = s,a_t=a]\\
 &= R(s,a)+ \gamma \sum_{s'\in S}P(s'|s,a)V^{\pi}(s') \\
 &= R(s,a)+ \gamma \sum_{s'\in S}P(s'|s,a) \sum_{a' \in A} \pi(a'|s')Q^{\pi}(a',s')
\end{align*}

\item In  \textbf{matrix form} using induced MRP (N.B.: max eigenvalue $\gamma P^{\pi} = 1 \cdot \gamma = \gamma$)
$$ V^{\pi} = R^{\pi} + \lambda P^{\pi}V^{\pi}$$
$$ V^{\pi} = (I - \gamma P^{\pi})^{-1}R^{\pi}$$
\begin{itemize}
\item $\gamma \leq 1$ matrix is \textbf{not singular}
\item $\gamma =1$ matrix is \textbf{singular}
\end{itemize}
\end{itemize}

\subsubsection{Bellman Operators}
\begin{itemize}
\item Bellman operator for $V^{\pi}$ is defined as $T^{\pi} : R^{|S|} x R^{|S|}$ (maps value functions to value functions)
$$(T^{\pi}V^{\pi})(s) = \sum_{a \in A} {\pi}(a|s)\left( R(s,a) + \gamma\sum_{s' \in S} P(s'|s,a)V^{\pi}(s') \right)$$

\item Bellman operator for $Q^{\pi}$ is defined as $T^{\pi} : R^{|S|x|A|} x R^{|S|x|A|}$ (maps action- value functions to action-value functions)
$$(T^{\pi}Q^{\pi})(s,a) =  R(s,a) + \gamma\sum_{s' \in S} P(s'|s,a) \sum_{a \in A} {\pi}(a'|s')Q^{\pi}(s',a')$$

\item Expectation equation become:
$$ T^{\pi}V^{\pi} = V^{\pi}$$
$$ T^{\pi}Q^{\pi} = Q^{\pi}$$

\item \textbf{Properties}
\begin{itemize}
\item \textbf{Monotonicity} : if $f_1 \leq f_2 $
$$ T^{\pi}f_1 \leq T^{\pi}f_2 \quad T^{*}f_1 \leq T^{*}f_2$$ 
\end{itemize}

\item $V^{\pi}$ is a \textbf{fixed point} of the operator, so any other input will result in a vector different from $V^{\pi}$. In the space of vectors ,each point is a different vector with its state components. Somewhere there is a vector $V^{\pi}$ which has a closed form solution

\item If $0 < \gamma < 1$ then $T^{\pi}$ is a \textbf{contraction} wrt to the \textbf{maximum norm} : only 1 fixed points,all other points moved towards it.

\item Max-norm contractions for two vectors $f_1,f_2$
$$ ||T^{\pi}f_1 - T^{\pi}f_2||_{\infty} \leq ||f_1-f_2||_{\infty}$$
$$ ||T^{*}f_1 - T^{*}f_2||_{\infty} \leq ||f_1-f_2||_{\infty}$$

\item $V^{\pi},V^{*}$ are fixed points of $T^{\pi},T^{*}$

\item For any vector $f \in R^{|S|}$ and policy $\pi$
$$ \lim_{k \to \infty } (T{\pi})^{k}f= V^{\pi}$$
$$ \lim_{k \to \infty } (T{*})^{k}f= V^{*}$$


\subsubsection{Optimality functions and operators}

\item Optimal policy $\pi^{*}$ : a policy that is better than or equal to all other policies
$$ \pi^{*} \geq \pi \quad \forall \pi$$
\begin{itemize}
\item $\pi^{*}$ achieves \textbf{optimal value function} $V^{\pi^*}=V^*$
\item $\pi^{*}$ achieves \textbf{optimal action function} $Q^{\pi^*}=Q^*$
\item There is always one for any MDP
$$ \pi^*(a|s)= \begin{cases}
1 & \quad \text{if } a=argmax_{a \in A}Q^*(s,a)\\
0 & \quad \text{otherwise} 
\end{cases}$$
\end{itemize}
\item \textbf{Optimal state value function} $V^{*}$ is the maximum value function over all polices
$$ V^{*} = max_{\pi}V^{\pi}(s)$$

\item \textbf{Optimal action-value function} $Q^{*}$ is the maximum action value function over all policies
$$ Q^{*}(s,a)= max_{\pi}(s,a)$$


\item \textbf{Bellman optimality equation for V}
$$ V^{*}(s) = max_a\left\{  R(s,a) +\gamma \sum_{s' \in S}P(s'|s,a)V^*(s')  \right\}$$
\item \textbf{Bellman optimality equation for Q}
$$ Q^{*}(s,a) = R(s,a) +\gamma \sum_{s' \in S}P(s'|s,a)max_aQ^*(s',a')$$
\end{itemize}

\subsubsection{Solution strategies}
\begin{itemize}
\item Bellman optimality operator non-linear so no closed form solution but many iterative methods 
\begin{itemize}
\item Dynamic programming : \textbf{Policy Iteration} , \textbf{Value Iteration}
\item Linear programming 
\item Reinforcement Learning :	\textbf{Q-Learning} , \textbf{SARSA}
\end{itemize}
\end{itemize}


\section{Solving MDPs}

\subsection{Policy Search}
\begin{itemize}
\item Solving MDPs = finding \textbf{optimal policy}
\item \textbf{Brute force} naive approach :
\begin{itemize}
\item enumerate all the Markovian policies
\item evaluate each policy
\item retrun best one
\end{itemize}

\item Brute force complexity : $|A|^{|S|}$. Improved with
\begin{itemize}
\item Restrict search to subset of possible best policies
\item Use \textbf{stochastic} optimization algorithm
\end{itemize}
\end{itemize}

\subsection{Dynamic Programming}
Recursively solves \textbf{subproblems} and then combines the solution.
\begin{itemize}
\item \textbf{Optimal substructure} : optimal solutions can be decomposed into subproblems ( \textbf{MDPs Bellman Equation})

\item \textbf{Overlapping subproblems}: subproblems may recur many times so solutions can be cached and reused ( MDPs \textbf{Value-Function})

\item \textbf{Fully known MDPs} used for
\begin{itemize}
\item \textbf{Prediction} : 
\begin{itemize}
\item input $\langle S,A,R,P,\gamma ,\mu \rangle$ 
\item policy $MRP \langle S, P^{\pi},R^{\pi},\gamma,\mu \rangle $ 
\item output: \textbf{value function } $\bm{V^{\pi}}$ 
\end{itemize}

\item \textbf{Control}:
\begin{itemize}
\item input $MRP \langle S, P^{\pi},R^{\pi},\gamma,\mu \rangle $ 
\item output : value-function $V^*$ and optimal policy $\pi^*$
\end{itemize}
\end{itemize}

\item Case of \textbf{finite-horizon} MDPs (non-stationary policy!) : use backward induction
\begin{itemize}
\item \textbf{Backward recursion} 
$$ 	V_k^* (s) = max_{a \in A_k} \left\{ R_k(s,a) + \sum_{s' \in S_{k+1}} P_k(s'|s,a) V^*_{k+1}(s') \right\} \quad k= N-1,...0$$

\item \textbf{Optimal policy}
$$ 	\pi^* (s) = max_{a \in A_k} \left\{ R_k(s,a) + \sum_{s' \in S_{k+1}} P_k(s'|s,a) V^*_{k+1}(s') \right\} \quad k= 0,...N-1$$
\item Total cost $N|S||A|$ vs $|A|^{N|S|}$ of brute search
\end{itemize}
\end{itemize}


\subsubsection{Policy Iteration}
For a given policy $\pi$ compute $V^{\pi}$
\begin{itemize}
\item State value function for policy $\pi$ 
$$ V^{\pi} (s) = E \left\{ \sum_{t=0}^{\infty} \gamma^tr_t|s_0=s \right\}$$

\item Bellman Equation for $V^{\pi}$
$$ V^{\pi} (s)= \sum_{a \in A} \pi(a|s) \left[ R(s,a) + \gamma \sum_{s' \in S}P(s'|s,a)V^{\pi}(s') \right]$$

\item Closed form solution $V^{\pi} = (I-\gamma P^{\pi})^{-1}R^{\pi} \rightarrow$ \textbf{huge complexity}

\item \textbf{Policy iteration} : \textbf{policy evaluation} + \textbf{policy improvement} 

\item \textbf{Policy evaluation}: full policy evaluation backup
$$ V_{k+1} (s) \leftarrow \sum_{a \in A} \pi(a|s) \left[ R(s,a) + \gamma \sum_{s' \in S}P(s'|s,a)V_k(s') \right]$$
\begin{itemize}
\item With \textbf{sweep} : state backup to update only modified states
\item After a few iterations, even if optimal value function is not found,the \textbf{optimal policy} has usually converged (depends on shape of V).This is also what makes it faster than \textbf{Value Iteration}
\end{itemize}

\item \textbf{Policy improvement} : consider a deterministic policy $\pi$ and a given state s, would it be better to perform an action $a \neq \pi(s)$? By acting greedily : 
$$ \pi'(s) = argmax_{a in A}Q^{\pi}(s,a)$$
$$ Q^{\pi}(s, \pi'(s))=max_{a \in A}Q^{\pi}(s,a) \geq Q^{\pi}(s,\pi(s))= V^{\pi}(s)$$
\textbf{Policy improvement theorem
}\begin{center}
\fbox{\begin{minipage}{30em}
Let $\pi$ and $\pi'$ be any pair of deterministic policies such that 
$Q^{\pi}(s,\pi'(s)) \geq V^{\pi}(s) , \forall s \in S$ , then policy $\pi'$ must be \textbf{as good or better} than  $\pi$ , $V^{pi'} \geq V^{\pi}, s \in S$
\end{minipage}}
\end{center}
Proof 
\begin{align*}
V^{\pi}(s) &\leq Q_{\pi}(s,\pi'(s)) = E_{\pi'}[r_{t+1} + \gamma V^{\pi}(s_{t+1})|s_t=s] \\
&\leq E_{\pi'}[r_{t+1} + \gamma Q^{\pi}(s_{t+1},\pi'(s_{t+1})|s_t=s]\\
&\leq E_{\pi'}[r_{t+1} \gamma r_{t+2}+ \gamma^2 Q^{\pi}(s_{t+2},\pi'(s_{t+2})|s_t=s]\\
&\leq E_{\pi'}[r_{t+1} \gamma r_{t+2}+... |s_t=s]= V^{\pi'}(s)
\end{align*}

\item If improvement stop $V^{\pi} = V^{\pi'} \rightarrow$ \textbf{Bellman optimality equation} 
$$ V^{\pi} = V^{\pi'} = V^*$$
$$ \pi = \pi^*$$ 
\end{itemize}

\subsubsection{Value iteration}
Find the \textbf{optimal policy} by iteratively applying \textbf{Bellman Optimality Equation} without an explicit policy.

\begin{itemize}
\item 	max norm $||V||_{\infty} = max_{s \in S} |V(s)|$
\end{itemize}
\begin{center}
\fbox{\begin{minipage}{30em}
Value iteration converges to the optimal state-value function $\lim_{x \to \infty}V_k = V^*$
$$ ||V_{k+1}-V^*||_{\infty} = ||T^*V_k - T^*V^*||_{\infty} \leq \gamma||V_k - V^*||_{\infty} \leq $$ $$... \leq \gamma^{k+1}||V^0 - V*||_{\infty} \rightarrow \infty$$
\end{minipage}}
\end{center}

\begin{center}
\fbox{\begin{minipage}{30em}
$$ ||V_{i+1}-V_i||_{\infty} < \epsilon \implies ||V_{i+1} - V^*||_{\infty} < \frac{2\epsilon}{1-\gamma}$$
\end{minipage}}
\end{center}

\subsection{Infinite Horizon Linear Programming}
\begin{itemize}
\item Value iteration convergence: 
$$ V^{*}(s) = max_a\left\{  R(s,a) +\gamma \sum_{s' \in S}P(s'|s,a)V^*(s')  \right\}$$

\item LP formulation for $V^*$:
\begin{itemize}
\item min$_v \sum_{s \in S} \mu(s)V(s)$
\item subject to : $V(s) geq R(s,a)+ \sum_{s' \in S}P(s'|s,a)V(s') \quad \forall s \in S , \forall a \in A$ 
\end{itemize}
\item $|S|$ variables , $|S||A|$ constraints

\item \textbf{Optimal Bellman Operator} $T^*$ 
\begin{itemize}
\item min$_v \mu^I V$
\item s.t. $V \geq T^*V$
\end{itemize}
Thanks to the monotonicity property $ U \geq V \rightarrow T^*U \geq T^*V$ and by repeated application $V \geq T^*V \geq T^{*^2}(V) \geq T^{*^3}(V)... \geq T^{*^\infty}(V) = V^*$. Since any feasible solution must satisfy $V \geq T^*(V)$ it satisfies also $V \geq V^*$ 
\end{itemize}

\subsubsection{Dual Problem}
\begin{itemize}
\item $$max_{\lambda} \sum_{s \in S} \sum_{a \in A} \lambda(s,a)R(s,a)$$
\item s.t. $$\sum_{a' \in A} \lambda(s',a')= \mu(s) + \gamma \sum_{s \in S} \sum_{a \in A}\lambda(s,a)P(s'|s,a), \forall s' \in S$$
\item s.t. $$\lambda \geq 0  \forall s \in S , a \in A$$

In this case $\lambda(s,a) = \sum_{t=0}^{\infty} \gamma^tP(s_t=s,a_t=a)$.\\
The optimal policy is	
$$ \pi^* (s) = argmax_a \lambda(s,a)$$
\end{itemize}


\section{RL in finite domains}
Model is \textbf{not known} (\textbf{model free)} but it is possible to interact with the environment
\begin{itemize}
\item \textbf{Model-free prediction}: \textbf{estimate} value function of an unknown MDP ( MDP +policy)
\item \textbf{Model-free control:} \textbf{optimize} the value function of an unknown MDP
\end{itemize}

\section{Monte Carlo RF}
\begin{itemize}
\item  Model-Free : no known MDP
\item  Learns directly from experience : \textbf{complete} episodes ( no bootstrapping)
\item Simple idea \textbf{value=mean return}
\item Works only with \textbf{episodic MDPs} where all episodes \textbf{terminate}
\end{itemize}
Can be used for
\begin{itemize}
\item Prediction 
\begin{itemize}
\item \textbf{Input}  : episodes of experience $\{s_1,a_1,r_2,...s_T\}$ generated by policy $\pi$
\item \textbf{Output} : value function $V^{\pi}$
\end{itemize}
\item Control
\begin{itemize}
\item \textbf{Input}  : episodes of experience $\{s_1,a_1,r_2,...s_T\}$ generated by policy $\pi$
\item \textbf{Output} : optimal value function $V^{*}$
\item \textbf{Output} : optimal policy $\pi^*$
\end{itemize}
\end{itemize}

\subsection{Monte-Carlo Prediction}
\begin{itemize}
\item Goal : estimate value function for a given policy  by averaging the returns observed after visits to states. As more returns are observed the average should \textbf{converge} to the \textbf{expected value}

\item X is R.V. with mean $\mu$ and variance $\sigma^2$
\begin{itemize}
\item \textbf{Empirical mean}:$$ \hat{\mu}_n = \frac{1}{n}\sum_{i=0}^n x_i$$	
\item $E[\hat{\mu}_n] = \mu $ , $Var[\hat{\mu}_n]= \frac{Var[X]}{n}$
\item Weak law ($\hat{\mu}_n \rightarrow^{P} \mu$) and string law ($\hat{\mu}_n \rightarrow_{a.s.} \mu $) of  large numbers
\end{itemize}

\item Policy evaluation of $V^{\pi}$ uses \textbf{empirical mean} instead of expected return and can be found using \textbf{two-approaches}
\begin{itemize}
\item \textbf{First-Visit MC}: average returns only for the first time s is visited (\textbf{unbiased estimator} ) in an episode
\item \textbf{Every-Visit-MC} : average returns for every time s is visited (\textbf{biased but consistent})
\end{itemize}

\item Mean can be computed \textbf{incrementally} : 
$$ \hat{\mu}_k = \frac{1}{k} \sum_{j=1}^k x_j = \frac{1}{k}\left( x_k + \sum_{j=1}^{k-1}x_j \right)= \frac{1}{k}(x_k+(k-1)\hat{\mu}_{k-1})=\hat{\mu}_{k-1}+\frac{1}{k}(x_k - \hat{\mu}_{k-1})$$

\item This can be used for \textbf{incremental updates}, for each state s with return $v_t$ and being N the time-steps s has been visited:
$$ N(s_t) \leftarrow N(s_t)+1$$
$$ V(s_t) \leftarrow V(s_t)+ \frac{1}{N(s_t)}(v_t-V(s_t))$$
(Discounted return - Expected reward)

\item A \textbf{running mean} should be used in \textbf{non-stationary problems} , which may no converge though : 
$$ V(s_t) \leftarrow V(s_t) + \alpha(v_t - V(s_t))$$
\end{itemize}

\subsubsection{Stochastic Approximation of Mean Estimator}
Let X be random variable in $[0,1]$ with mean $\mu=E[x]$. Let $x_i \sim X,i=1...n$ iid realizations of X.\\
The estimator (\textbf{exponential average}) 
$$ \mu_i=(1-\alpha_i)\mu_{i-1}+a_i\mu_i$$
with $\mu_1 = x_1$ and $a_i$ learning rates

\begin{itemize}
\item if $\sum_{i \geq 0 }\alpha_i= \infty$ and $\sum_{i \geq 0 }\alpha^2_i < \infty$ then $$ \hat{\mu}_i \rightarrow_{a.s.} \mu$$ which means that the estimator $\hat{\mu}_n$ is \textbf{consistent}
\end{itemize}

\section{Temporal Difference Learning}
\begin{itemize}
\item  Model-Free : no known MDP
\item  Learns directly from experience : \textbf{incomplete} episodes ( bootstrapping)
\item Updates guess with a guess
\end{itemize}

\subsection{TD Prediction}
\begin{itemize}
\item Learn $V^{\pi}$ online from experience under policy $\pi$
\item Monte-Carlo incremental update $ V(s_t) \leftarrow V(s_t) + \alpha(v_t - V(s_t))$
\item Simplest Temporal Difference learning algorithm \textbf{TD(0)} : update value $V(s_t)$ towards \textbf{estimated return} $r_{t+1} + \gamma V(s_{t+1})$
$$ V(s_t) \leftarrow V(s_t) + \alpha(\bm{r_{t+1}+\gamma V(s_{t+1})} - V(s_t))$$
where 
\begin{itemize}
\item $r_{t+1}+ \gamma V(s_{t+1})$ is the \textbf{TD - Target}
\item $\delta_t = r_{t+1}+ \gamma V(s_{t+1}) - V(s_t)$ is the \textbf{TD-Error}
\end{itemize}
\end{itemize}

\subsection{MC vs TD}
\textbf{TD}
\begin{itemize}
\item learns \textbf{before} knowing final outcome or \textbf{without} the final outcome
\item learn \textbf{online} at every step
\item can learn from \textbf{incomplete} episodes
\item works in \textbf{continuing} non-terminating MDPs
\item \textbf{low variance ,some bias}:
\begin{itemize}
\item \textbf{TD-target} is a \textbf{biased} estimate of $V^{\pi}$ unless $V^{\pi}(s_{t+1}) = V(s_{t+1})$
\item \textbf{TD-target} has much lower variance as target depends on \textbf{one random} action,transition and reward
\end{itemize}
\item \textbf{TD(0)} converges to $V^{\pi}(s)$
\item Is sensitive to initial values
\end{itemize}
\textbf{MC}
\begin{itemize}
\item must wait \textbf{until end of episode} before return is known
\item learns only from \textbf{complete} sequences
\item works only for \textbf{episodic} MDPs
\item \textbf{high variance ,no bias}:
\begin{itemize}
\item the return $v_t $is an \textbf{unbiased} estimate of $V^{\pi}$ 
\item return $v_t$ has high variance as it depends on \textbf{many random} actions,transitions and rewards
\end{itemize}
\item \textbf{Not} sensitive to initial values
\end{itemize}
\textbf{Batch updating}
\begin{itemize}
\item MC and TD converge for experience $\to \infty$
\item If finite amount of experience : repeat experience, given a V the increments are computed a computed at every time step t at \textbf{non-terminal} states but \textbf{value function} is changed only once, by the sum of all increments.This is repeat until convergence.
\begin{itemize}
\item TD(0) \textbf{converges} deterministically to a \textbf{single answer} independent of $\alpha$ (which must be sufficiently small ). It converges to the \textbf{max likelihood} Markov model
$$ \hat{P}(s'|s,a)= \frac{1}{N(s,a)}\sum_{k=1}^K\sum_{t=1}^T \bm{1}(s^k_t, a^k_t, s^k_{t+1}= s,a,s')$$
$$ \hat{R}(s,a)= \frac{1}{N(s,a)}\sum_{k=1}^K\sum_{t=1}^T \bm{1}(s^k_t, a^k_t = s,a)r_t^k$$
\item MC constant $\alpha$ also converges in the same conditions but to a \textbf{different answer}. It converges to the \textbf{minimum mean squared error}
$$ \sum_{k=1}^K\sum_{t=1}^T (v_t^k - V(s_t^k))^2$$
\end{itemize}
\item Markov \textbf{memoryless} property :
\begin{itemize}
\item  exploited by TD(0) $\rightarrow$ works better in Markovian environments
\item not exploited by MC $\rightarrow$ works better in Non-Markovian environments
\end{itemize}
\end{itemize}

\subsection{\textit{n}-Step TD Prediction}
\begin{itemize}
\item \textit{n-steps} enable \textbf{bootstrapping} to occur over multiple steps
$$\begin{matrix}
 n=1 &(TD) & \quad &  v_t^{(1)}= r_{t+1}+\gamma V(s_{t+1}) \\
 n=2 	& & \quad &v_t^{(2)} = r_{t+1}+\gamma r_{t+2} + \gamma^2 V(s_{t+2})\\
 \vdots & & \quad &\vdots \\
 n=\infty &(MC) &\quad &  v_t^{(\infty)} = r_{t+1}+\gamma r_{t+2} + \hdots+ \gamma^{T-1}r_T)
\end{matrix}$$
$$ \text{n-step Return: } v_t^{(n)}=r_{t+1}+\gamma r_{t+2}+ \hdots +\gamma^{n-1}r_{t+n}+ \gamma^n V(s_{t+n})$$
\item \textit{n}-step TD learning
$$ V(s_t) \leftarrow V(s_t)+\alpha(v_t^{(n)}-V(s_t))$$

\item \textit{n}-steps can be \textbf{averaged} over \textbf{different n}.E.g. average 2-step and 4-step return 
$$ \frac{1}{2}v^{(2)} + \frac{1}{2}v^{(4)}$$
which combines information about 2 different time steps. More time-steps can be combined using \textbf{lambda return}

\item $\bm{\lambda}$ \textbf{return} $\bm{v_t^{\lambda}}$ combines all \textit{n}-step returns ,each weighted proportional to $\lambda^{n-1}$ and is normalized by a factor of $1-\lambda$ to ensure they sum up to 1
$$ v_t^{\lambda}= (1-\lambda) \sum _{n=1}^\infty \lambda^{n-1}v_t^{(n)}$$

\begin{itemize}
\item \textbf{Forward-view} $TD(\lambda)$  look into the future to compute the return. For each state visited, look forward in time to all future rewards and decide how best to combine them: $$V(s_t) \leftarrow V(s_t)+\alpha(v_t^{\lambda} - V(s_t))$$
Need whole episodes like MC

\item \textbf{Backward-view} $TD(\lambda)$ updates weight vector on every step of an episode rather than only at the end, and thus it estimates may be better \textbf{sooner}.The computations are \textbf{equally distributed over time} rather than at the end of each episode . Finally it can be applied to \textbf{incomplete episodes}
$$ e_0(s) =0 ,\quad e_t(s) = \gamma \lambda e_{t-1}(s) + \bm{1}(s=s_t)$$
$$ V(s) \leftarrow V(s) + \alpha \delta_te_t(s)$$
Where $e_t$ is an \textbf{eligibility trace} a heuristic that combines \textbf{frequency} and \textbf{recency} heuristic. \\$delta_t$ is the TD error.\\
$\lambda$ is the \textbf{trace decay} parameter that tells at which rate the trace falls.
\begin{itemize}
\item $\lambda =0 $ only current state is updated , which is equivalent to $TD(0)$
\item $\lambda = 1$ the sum \textbf{telescopes} into MC Error. In this case it is equivalent to MC Every Visit if \textbf{updated offline}. If update \textbf{online} the answer can be different.

\item Convergence problems can be solved by setting the trace to 1 instead of adding 1
$$e_t(s) =
\begin{cases}
\gamma\lambda e_{t-1}(s) &\quad s \neq s_t\\
1 &\quad s = s_t
\end{cases}$$
\end{itemize}
\end{itemize}
\end{itemize}

\section{Model-Free Control}
\begin{itemize}
\item Model free control solves problems where model is unknown but experience can be sampled or the model is known but too complex to use except for the experience.

\item How can learning the \textbf{optimal policy} be achieved while acting in a sub-optimal way in order to \textbf{explore all actions}?
\begin{itemize}
\item \textbf{On-Policy} : learn about policy $\pi$ from experience samples from $\pi$. It learns action-values for \textbf{near optimal policies}
\item \textbf{Off-Policy} : learn about policy $\pi$ sampled from $\bar{\pi}$. Target policy is \textbf{different} than behavioural policy
\end{itemize}
\end{itemize}

\subsection{On-Policy}

\subsubsection{On-policy : Monte-Carlo Control}
\begin{itemize}
\item \textbf{Policy evaluation} performed on $Q(s,a)$ instead of $V(s)$ because $V(S)$ \textbf{requires a model} for \textbf{greedy policy improvement}
$$ \pi' = argmax_{(a \in A )}\{ R(s,a) + P(s'|s,a)V(s')\}$$
$$ vs.$$
$$ \pi' = argmax_{(a \in A)} Q(s,a)$$

\item \textbf{Policy improvement} performed using $\bm{\epsilon}-greedy$ exploration that ensures \textbf{continual} exploration where all \textbf{m-actions} are tried with \textbf{non-zero probability}
$$ \pi(s,a) =
\begin{cases}
\frac{\epsilon}{m}+1 - \epsilon & \quad \text{if } a^* =argmax_{(a \in A)}Q(s,a) \\
\frac{\epsilon}{m} & \quad \text{otherwise}
\end{cases}
$$

\begin{center}
\fbox{\begin{minipage}{30em}
For any $\epsilon$-greedy policy $\pi$, the $\epsilon$-greedy policy $\pi'$ wrt to $Q^{\pi}$ is an \textbf{improvement}
\begin{align*}
Q^{\pi}(s,\pi'(s)) &=\sum_{a \in A}\pi'(a|s)Q^{\pi}(s,a)\\
&=  \frac{\epsilon}{m} \sum_{a \in A}Q^{\pi}(s,a) + (1-\epsilon)max_{a \in A}Q^{\pi}(s,a)\\
&\geq \frac{\epsilon}{m} \sum_{a \in A}Q^{\pi}(s,a) + (1-\epsilon)max_{a \in A}\frac{\pi(a|s) - \frac{\epsilon}{m}}{1-\epsilon}Q^{\pi}(s,a)\\
&= \sum_{a \in A}\pi(a|s)Q^{\pi}(s,a) = V^{\pi}(s)
\end{align*}
Therefore $V^{\pi'} \geq V^{\pi}$ 
\end{minipage}}
\end{center}
 
\item In this scenario the policy evaluation \textbf{approximates} the unknown real $Q^{\pi} \rightarrow Q \approx Q^{\pi}$

\item Policies are labeled as \textbf{GLIE} ( Greedy in the limit of infinite exploration ) if they satisfy the conditions
\begin{itemize}
\item All state-action pairs are explored \textbf{infinitely } many times 
$$ \lim{k \to \infty} N_k(s,a) = \infty$$
\item The policy converges to a \textbf{greedy policy} 
$$ \lim{k \to \infty} \pi_k(a|s) = \bm{1}(a= argmax_{(a \in A)}Q_k(s',a')$$
\end{itemize}

\item Sample k-th episode using $\pi : \{ s_1,a_1,r_2,...,s_T\} \sim \pi$. For each state-action in the episode 
$$ N(s_t,a_t) \leftarrow N(s_t,a_t)+1$$
$$ Q(s_t,a_t) \leftarrow Q(s_t,a_t)+ \frac{1}{N(s_t,a_t)}(v_t - Q(s_t,a_t))$$
Improve policy based on new action-value function 
$$ \epsilon \leftarrow \frac{1}{k}$$
$$ \pi \leftarrow \epsilon-greedy(Q)$$  

\begin{center}
\fbox{\begin{minipage}{30em}
GLIE Monte-Carlo control \textbf{converges} to the \textbf{optimal} action-value function  $Q(s,a) \leftarrow Q^*(s,a)$
\end{minipage}}
\end{center}

\item Relevant time scales
\begin{itemize}
\item Behavioural time-scale $\frac{1}{1-\gamma}$ discount factor
\item Sampling in the estimation of Q : $\alpha$ learning rate
\item Exploration $\epsilon$
$$ 1-\gamma >> \alpha >> \epsilon$$
$$ \text{Initially : } 1-\gamma \approx \epsilon \approx \alpha$$
Then decrease $\epsilon$ faster than $\alpha$
\end{itemize}
\end{itemize}

\subsubsection{On-Policy : TD with SARSA}
\begin{itemize}
\item Many advantages over MC
\begin{itemize}
\item Lower variance
\item Online
\item Incomplete sequences
\end{itemize}
\item Uses update rule 
$$ Q(s,a) \leftarrow Q(s,a) 	+ \alpha(r+ \gamma Q(s',a')-Q(s,a))$$
which is done after every transition from a non-terminal state s. If s' is terminal $Q(s',a') = 0$
\begin{center}
\fbox{\begin{minipage}{30em}
SARSA converges to the \textbf{optimal} action-value function $Q(s,a) \rightarrow Q^*(s,a)$ if
\begin{itemize}
\item GLIE sequence of policies $\pi_t(s,a)$
\item \textbf{Robbin-Monroe} sequence of steps-size $\alpha_t$
$$ \sum_{t=1}^{\infty} \alpha_t = \infty$$
$$ \sum_{t=1}^{\infty} \alpha_t^2 < \infty$$
\end{itemize}
\end{minipage}}
\end{center}

\item \textbf{Policy Evaluation} : \textbf{SARSA}
\item \textbf{Policy improvement} : $\bm{\epsilon-greedy}$ 

\item  $\lambda$ return is supported by SARSA as well
\begin{itemize}
\item \textbf{Forward view} : update Q(s,a) to $\lambda-\text{return } v_t^{\lambda}$ 
\item \textbf{Backward view} : incorporate eligibility traces for state-action pairs
$e_t(s,a)= \gamma \lambda e_{t-1}(s,a) +\bm{1}(s_t,a_t = s,a)$ 
\end{itemize}
\end{itemize}

\subsection{Off-Policy}
\begin{itemize}
\item Learn about \textbf{target policy} $\pi(a|s)$ while following behavioural policy $\bar{\pi}(a|s)$
\item Use \textbf{Importance sampling} : estimate expectation of a different distribution wrt the distribution used to draw samples
$$ E_{x \sim P}[f(x)] = \sum P(x)f(x) = \sum Q(x)\frac{P(x)}{Q(x)}f(x)= E_{x \sim Q}[\frac{P(x)}{Q(x)}f(x)]$$
\end{itemize}

\subsubsection{Off-Policy : Monte Carlo}
\begin{enumerate}

\item Uses return generated from $\bar{\pi}$ to evaluate $\pi$
\item Weight return $v_t$ calculated according to \textbf{similarity} between policies
\item Multiple \textbf{importance sample corrections} along with episode : 
$$ v_t^{\mu} = \frac{\pi(a_t|s_t)}{\bar{\pi}(a_t|s_t)}\frac{\pi(a_{t+1}|s_{t+1})}{\bar{\pi}(a_{t+1}|s_{t+1})}...\frac{\pi(a_T|s_T)}{\bar{\pi}(a_T|s_T)}v_t$$
\item Update towards \textbf{corrected return}
$$ Q(s_t|a_t)\leftarrow Q(s_t|a_t) + \alpha(\bm{v_t} -Q(s_t,a_t))$$
Cannot be used where $\bar{\pi}$ = 0 and $\pi$ is not zero
\end{enumerate}
\begin{itemize}

\item Importance sampling can \textbf{dramatically} increase \textbf{variance}
\end{itemize}

\subsubsection{Off-Policy : TD with SARSA}
\begin{enumerate}
\item Use \textbf{TD Targets} generated from $\pi$ to evaluate $\bar{\pi}$
\item Weight TD target $r+\gamma Q(s',a')$ according to \textbf{similarity} between policies
\item Use \textbf{single} importance sampling correction
$$ Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \left( r_{t+1} + \gamma \frac{\pi(a|s)}{\bar{\pi}(a|s)}Q(s_{t+1},a_{t+1}) - Q(s_t,a_t) \right)$$
\end{enumerate}

\begin{itemize}
\item Has much lower variance than MC
\end{itemize}

\subsubsection{Off-Policy : TD with Q-Learning}
\begin{itemize}
\item Learn about optimal policy $\pi=\pi*$
\item From experience sampled from behaviour policy $\bar{\pi}$ , estimate
$$ Q(s,a) \approx Q^*(s,a)$$
\item Uses update rule 
$$ Q(s,a) \leftarrow Q(s,a)+\alpha(r+ \gamma max_{a \in A}Q(s',a')-Q(s,a))$$

\item It is possible to use \textbf{eligibility traces} with Q-Learning using \textbf{Watkins}
\begin{itemize}
\item Zero-out eligibility traces after a non-greedy action
\item Do max when backing up at first non-greedy choice
\end{itemize}
$$ e_t(s,a) = \begin{cases}
1+\gamma \lambda e_{t-1}(s,a) \quad & if s=s_t,a=a_t,Q_{t-1}(s_t,a_t)= max_aQ_{t-1}(s_t,a)\\
0 \quad & if Q_{t-1}(s_t,a_t) \neq max_aQ_{t-1}(s_t,a)\\
\gamma\lambda e_{t-1}(s,a) \quad & otherwise
\end{cases}$$
This gives an \textbf{disadvantage} in early learning as the eligibility trace will be \textbf{zeroed-out} frequently. A solution is to use \textbf{Peng's} Q-Learning (hard to implement)
\begin{itemize}
\item  Back-up max action \textbf{at the end}
\item Never cut traces
\end{itemize}
\end{itemize}

\section{Multi-Armed Bandit}
\begin{itemize}
\item $Q(s,a)$ often not known and online \textbf{decision making} faces two challenges : \textbf{exploration} vs \textbf{exploitation}. Depending on the situation sometimes gaining in the future by making sacrifices in the present is useful.
\item Exploration is done for example with
\begin{itemize}
\item $\epsilon-greedy$  : does not achieve optimal policy
\item \textbf{Softmax-Bolzman Distribution} 
$$ \pi(a_i|s) = \frac{e^{\hat{Q}(\frac{a_i|s}{\tau})}}{\sum_{a \in A}e^{\frac{\hat{Q}(a|s)}{\tau}}}$$
which weights the action according to $\hat{Q}(s,a)$ and $\tau$ is a \textbf{temperature} parameter that increases over time. This approach selects with high probability an action with high reward.
\end{itemize}

\item MAB is a special case of MDP
\begin{itemize}
\item \textbf{Single } state S (expectation of reward will change over time stochastically )
\item Set of \textbf{arms} 
\item P Probability matrix (only 1 state , probability of staying in that state is $\bm{1}$)
\item R reward function (can be \textbf{deterministic}, \textbf{stochastic} , \textbf{adversary})
\item $\gamma=1$, $\mu=1$
\end{itemize}
\end{itemize}


\subsection{Stochastic MAB}
\begin{itemize}
\item A MAB setting is a tuple $\langle A,R \rangle$
\begin{itemize}
\item A set of N possible arms
\item R is a set of \textbf{unknown} distributions $r_{i,t}\sim R(a_i)$ and $E[R(a_i)] = R(a_i)$ (for simplicity can be assumed to be $\in [0,1]$)
\end{itemize}
\item Procedure is as follows:
\begin{enumerate}
\item Each time step t	agents selects single arm $a_{i_t}$
\item Environment generates reward $r_{i_t}$
\item Agents updates info by means of history $h_t$
\end{enumerate}
\item Aim is to \textbf{maximize cumulative reward}
$$ \sum_{t=1}^T r_{i_t,t}$$
\end{itemize}

\subsubsection{Regret}
\begin{itemize}
\item Assume best arm is $a^*$ with expected reward $R^*=R(a)=max_{a \in A}E[R(a)]$. Then the \textbf{loss} at each time step is
$$ R(a^*) - R(a_{i_t})$$
\item Over a time horizon T on \textbf{average} the loss (called \textbf{Expected Pseudo Regret}) is 
$$ L_T = TR^* -E[\sum_{t=1}^{T} R(a_{i_t})]$$

\item Maximization of reward = minimization of cumulative regret
\item Second formulation using \textbf{difference} between $a_i$ and optimal one $a^*$ 
$$ \Delta_i = R^* - R(a_i)$$
Being $N_t(a_i)$ the \textbf{number of times} arm $a_i$ has been pulled , the \textbf{regret} becomes 
$$ L_T = TR^* - E[\sum_{t=1} R(a_{i_t})]= E[\sum_{t=1} R^* - R(a_{i_t})]$$
$$ = \sum _{a \in A}E[N_t(a_i)](R^* - R(a_i)) = \sum_{a \in A}E[N_T(a_i)]\Delta_i$$
\end{itemize}

\subsubsection{Lower Bound}
\begin{itemize}
\item $\Delta_i $ implies that any algorithm performance is implied by the \textbf{similarity among arms}. The more they are similar = the more difficult the problem

\begin{center}
\fbox{\begin{minipage}{30em}
Given a MAB stochastic problem ,any algorithm satisfies 
$$ \lim_{T \to \infty } L_T \geq logT \sum_{a_i| \Delta_i >0} \frac{\Delta_i}{KL(R(a_i),R(a^*))}$$
\end{minipage}}
\end{center}
\end{itemize}

\subsubsection{Pure Exploitation Algorithm}
\begin{itemize}
\item Always select action $a_{i_t} =argmax_a \hat{R}_t(a)$ where the expected reward is 
$$ \hat{R}_t(a_i)= \frac{1}{N_t(a_i)} \sum_{j=1}^t r_{i,j} \bm{1}\{a_i =a_{i_j} \}$$

\item Might \textbf{not converge to optimal solution}
\item Not considering \textbf{uncertainty} in $\hat{R}_t(a) \rightarrow$ need to provide explicit bonus for \textbf{exploration}
\item Two formulations
\begin{itemize}
\item \textbf{Frequentist :} $R(a_1),...,R(a_N)$ unknown parameters and a policy selects each time step an arm	based on history
\item \textbf{Bayesian}: $R(a_1),...,R(a_N))$ are random variables with priors and a policy selects at each time step an arm based on	observation history.\\
The more \textbf{uncertain} the more we want the algorithm to explore the option
\end{itemize}
\end{itemize}

\subsubsection{Upper Confidence Bound}
\begin{itemize}
\item Instead of using empiric estimate use \textbf{upper bound} $U(a_i)$ over expected value $R(a_i)$
$$ U(a_i):= \hat{R}_t(a_i) + B_t(a_i) \geq R(a_i)$$
with \textbf{high probability}
\item Bound $B_t(a_i)$ will depend on how much information we have on the arm (for example number of times the arm has been pulled)

\item Upper bound can be set using \textbf{Hoeffeding Inequality Bound} : 
$X_1,...,X_t$ iid random variables with support in $[0,1]$  and identical mean 
$E[X_i]=: X$ ,$\bar{X}_t = \frac{\sum_{i=1}^t X_i}{t}$ the sample mean , then
$$ P(X > \bar{X}_t + u ) \leq e^{-2tu^2}$$
Applied to each arm
$$ P(R(a_i) > \hat{R}(a_i) + B_t(a_i) ) \leq e^{-2tB_t(a_i)^2}$$
Then it can be computed as:
\begin{enumerate}
\item Pick probability p that the value exceed the bound
$$ e^{-2N_t(a_i)B_t(a_i)^2} = p_t $$
\item Solve to find $B_t$
$$ B_t(a_i) = \sqrt{\frac{-log p_t}{2N_t(a_i)}}$$ 
\item Reduce value over time $p_t = t^{-4}$
$$ B_t(a_i) = \sqrt{\frac{2log t}{N_t(a_i)}}$$ 
\item Ensure to select optimal action as samples increase
$$ \lim_{t \to \infty }B_t(a_i) = 0 \implies \lim_{t \to \infty}U_t(a_i) = R(a_i)$$
\end{enumerate}

\item \textbf{UCB1} exploits upper bound. At each time step t:
\begin{itemize}
\item Compute $\hat{R}(a_i)$
\item Compute $B_t(a_i)$
\item Play arm $a_{i_t} = argmax_{a_i \in A}\left( \hat{R}_t(a_i) +B_t(a_i) \right)$
\end{itemize}

At finite time T the \textbf{expected total regret of the UCB1} applied to stochastic MAB is
$$ L_T \leq 8log T \sum_{i|\Delta_i > 0} \frac{1}{\Delta_i}+(1+\frac{\pi^2}{3})\sum_{i|\Delta_i > 0} \Delta_i$$


\item \textbf{Thompson Sampling} is a general \textbf{Bayesian} technique for online learning
\begin{enumerate}
\item Consider Bayesian prior for each arm $f_1,..,f_N$
\item At each round t, sample from each on the distributions $\hat{r}_1,...,\hat{r}_N$
\item Pull the arm $a_i$ with the \textbf{highest sampled value} $i_t= argmax_i \hat{r}_i$
\item Update prior with new info
\end{enumerate}
Example with prior $f_i(0)=Beta(1,1)$ for each arm $a_i$. Keep distribution $f_i(t)=Beta(\alpha_t,\beta_t)$ incorporating information form each arm $a_i$
\begin{itemize}
\item \textbf{Success}  : $f_i(t+1)=Beta(\alpha_t+1,\beta_t)$
\item \textbf{Failure}  : $f_i(t+1)=Beta(\alpha_t,\beta_t+1)$
\end{itemize}
At time T, the \textbf{expected total regret} of the algorithm is
$$ L_T \leq O\left(  \sum_{i|\Delta_i > 0} \frac{\Delta_i}{KL(R_{a_i},R(a^*))}(log T + log log T)\right)$$
\end{itemize}

\subsection{Adversarial MAB}
\begin{itemize}
\item Setting tuple $\langle A,R \rangle$
\item A set of N arms
\item R reward vector of $r_{i_t}$ rewards decided by an \textbf{adversarial player}\item Each step agent selects 1 arm and adversary chooses reward which the agents receives.
\item Goal : \textbf{maximize cumulative reward} 
$$ \sum_{t=1}^T r_{i_t,t}$$
\end{itemize}

\subsubsection{Regret}
\begin{itemize}
\item Cannot compare received gain with \textbf{optimal one} and we cannot use \textbf{deterministic} algorithms ( adversary decides reward!). So only \textbf{weak regret} can be defined , compared with  \textbf{best constant action}
$$ L_T  = max_i\sum_{t=1}^T r_{i,t} - \sum_{t=1}^T r_{i_t,t} $$
\end{itemize}
\subsubsection{Lower Bound}
Let \textbf{sup} be the supremum over all distribution of rewards such that for all $i \in \{ 1,...,N\}$ the rewards $r_{i,1}, ...,r_{i,j} \in 0,1$ are iid , and let \textbf{inf} be the infimum over all forecasters. Then
$$ \text{inf sup }E[L_T]\geq \frac{1}{20} \sqrt{TN}$$
where the expectation is taken wrt both \textbf{random generation rewards} and \textbf{internal randomization} of forecaster

\subsubsection{EXP3}
\begin{itemize}
\item Variation of \textbf{Softmax}
\item Probability of choosing an arm 
$$ \pi(a_i)= (1-\beta)\frac{w_t(a_i)}{\sum _j w_t(a_j)} +\frac{\beta}{N}$$
where
 $$ w_{t+1}(a_i) = \begin{cases} 
 w_t(a)e^{- \eta  \frac{r_{i,t}}{\pi_t(a_i)}} & \text{ if } a_i \text{ has been pulled}\\
 w_t(a_i) & \text{otherwise}  
\end{cases} 
  $$
  
 At time T the \textbf{expected total regret} of EXP3 algorithm is with $\beta=\eta = \sqrt{\frac{N log N }{(e-1)T}}$ 
 $$ E[L_T] \leq O(\sqrt{TN log N})$$
\end{itemize}

\end{document}

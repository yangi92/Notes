\BOOKMARK [1][-]{section.1}{Linear Regression}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Direct Approach - OLS}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Direct Approach - Gradient Optimization}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Discriminative approach - MLE}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{Regularization}{section.1}% 5
\BOOKMARK [2][-]{subsection.1.5}{Bayesian Regression}{section.1}% 6
\BOOKMARK [1][-]{section.2}{Classification}{}% 7
\BOOKMARK [2][-]{subsection.2.1}{Discriminant approach}{section.2}% 8
\BOOKMARK [3][-]{subsubsection.2.1.1}{Considerations}{subsection.2.1}% 9
\BOOKMARK [3][-]{subsubsection.2.1.2}{Multi-class problem}{subsection.2.1}% 10
\BOOKMARK [3][-]{subsubsection.2.1.3}{Least Squares for classification}{subsection.2.1}% 11
\BOOKMARK [3][-]{subsubsection.2.1.4}{Perceptron}{subsection.2.1}% 12
\BOOKMARK [2][-]{subsection.2.2}{Probabilistic Discriminative Approach}{section.2}% 13
\BOOKMARK [3][-]{subsubsection.2.2.1}{Multi-class using Softmax}{subsection.2.2}% 14
\BOOKMARK [1][-]{section.3}{Bias-Variance trade-off and model selection}{}% 15
\BOOKMARK [2][-]{subsection.3.1}{Free Lunch Theorem}{section.3}% 16
\BOOKMARK [2][-]{subsection.3.2}{Bias-Variance}{section.3}% 17
\BOOKMARK [2][-]{subsection.3.3}{Train-Test Errors}{section.3}% 18
\BOOKMARK [2][-]{subsection.3.4}{Model selection}{section.3}% 19
\BOOKMARK [2][-]{subsection.3.5}{Model Ensembles}{section.3}% 20
\BOOKMARK [1][-]{section.4}{PAC-Learning and VC Dimensions}{}% 21
\BOOKMARK [2][-]{subsection.4.1}{Intro and Version Space}{section.4}% 22
\BOOKMARK [2][-]{subsection.4.2}{PAC-Learning}{section.4}% 23
\BOOKMARK [2][-]{subsection.4.3}{VC Dimension}{section.4}% 24
\BOOKMARK [1][-]{section.5}{Kernel Methods}{}% 25
\BOOKMARK [2][-]{subsection.5.1}{Dual representation}{section.5}% 26
\BOOKMARK [2][-]{subsection.5.2}{Constructing Kernels}{section.5}% 27
\BOOKMARK [2][-]{subsection.5.3}{List of valid kernels}{section.5}% 28
\BOOKMARK [1][-]{section.6}{SVM}{}% 29
\BOOKMARK [2][-]{subsection.6.1}{Learning in SVM}{section.6}% 30
\BOOKMARK [3][-]{subsubsection.6.1.1}{Constraint optimization recap}{subsection.6.1}% 31
\BOOKMARK [3][-]{subsubsection.6.1.2}{Dual and Primal Problem}{subsection.6.1}% 32
\BOOKMARK [2][-]{subsection.6.2}{Prediction}{section.6}% 33
\BOOKMARK [2][-]{subsection.6.3}{Solution Techniques}{section.6}% 34
\BOOKMARK [2][-]{subsection.6.4}{Noisy data and Slack Variables}{section.6}% 35
\BOOKMARK [2][-]{subsection.6.5}{Bounds}{section.6}% 36
\BOOKMARK [1][-]{section.7}{Reinforcement Learning}{}% 37
\BOOKMARK [2][-]{subsection.7.1}{Markov Decision Process}{section.7}% 38
\BOOKMARK [3][-]{subsubsection.7.1.1}{Reward and Goals}{subsection.7.1}% 39
\BOOKMARK [3][-]{subsubsection.7.1.2}{Policies}{subsection.7.1}% 40
\BOOKMARK [3][-]{subsubsection.7.1.3}{Value Function}{subsection.7.1}% 41
\BOOKMARK [3][-]{subsubsection.7.1.4}{Bellman Equations}{subsection.7.1}% 42
\BOOKMARK [3][-]{subsubsection.7.1.5}{Bellman Operators}{subsection.7.1}% 43
\BOOKMARK [3][-]{subsubsection.7.1.6}{Optimality functions and operators}{subsection.7.1}% 44
\BOOKMARK [3][-]{subsubsection.7.1.7}{Solution strategies}{subsection.7.1}% 45
\BOOKMARK [1][-]{section.8}{Solving MDPs}{}% 46
\BOOKMARK [2][-]{subsection.8.1}{Policy Search}{section.8}% 47
\BOOKMARK [2][-]{subsection.8.2}{Dynamic Programming}{section.8}% 48
\BOOKMARK [3][-]{subsubsection.8.2.1}{Policy Iteration}{subsection.8.2}% 49
\BOOKMARK [3][-]{subsubsection.8.2.2}{Value iteration}{subsection.8.2}% 50
\BOOKMARK [2][-]{subsection.8.3}{Infinite Horizon Linear Programming}{section.8}% 51
\BOOKMARK [3][-]{subsubsection.8.3.1}{Dual Problem}{subsection.8.3}% 52
\BOOKMARK [1][-]{section.9}{RL in finite domains}{}% 53
\BOOKMARK [1][-]{section.10}{Monte Carlo RF}{}% 54
\BOOKMARK [2][-]{subsection.10.1}{Monte-Carlo Prediction}{section.10}% 55
\BOOKMARK [3][-]{subsubsection.10.1.1}{Stochastic Approximation of Mean Estimator}{subsection.10.1}% 56
\BOOKMARK [1][-]{section.11}{Temporal Difference Learning}{}% 57
\BOOKMARK [2][-]{subsection.11.1}{TD Prediction}{section.11}% 58
\BOOKMARK [2][-]{subsection.11.2}{MC vs TD}{section.11}% 59
\BOOKMARK [2][-]{subsection.11.3}{n-Step TD Prediction}{section.11}% 60
\BOOKMARK [1][-]{section.12}{Model-Free Control}{}% 61
\BOOKMARK [2][-]{subsection.12.1}{On-Policy}{section.12}% 62
\BOOKMARK [3][-]{subsubsection.12.1.1}{On-policy : Monte-Carlo Control}{subsection.12.1}% 63
\BOOKMARK [3][-]{subsubsection.12.1.2}{On-Policy : TD with SARSA}{subsection.12.1}% 64
\BOOKMARK [2][-]{subsection.12.2}{Off-Policy}{section.12}% 65
\BOOKMARK [3][-]{subsubsection.12.2.1}{Off-Policy : Monte Carlo}{subsection.12.2}% 66
\BOOKMARK [3][-]{subsubsection.12.2.2}{Off-Policy : TD with SARSA}{subsection.12.2}% 67
\BOOKMARK [3][-]{subsubsection.12.2.3}{Off-Policy : TD with Q-Learning}{subsection.12.2}% 68
\BOOKMARK [1][-]{section.13}{Multi-Armed Bandit}{}% 69
\BOOKMARK [2][-]{subsection.13.1}{Stochastic MAB}{section.13}% 70
\BOOKMARK [3][-]{subsubsection.13.1.1}{Regret}{subsection.13.1}% 71
\BOOKMARK [3][-]{subsubsection.13.1.2}{Lower Bound}{subsection.13.1}% 72
\BOOKMARK [3][-]{subsubsection.13.1.3}{Pure Exploitation Algorithm}{subsection.13.1}% 73
\BOOKMARK [3][-]{subsubsection.13.1.4}{Upper Confidence Bound}{subsection.13.1}% 74
\BOOKMARK [2][-]{subsection.13.2}{Adversarial MAB}{section.13}% 75
\BOOKMARK [3][-]{subsubsection.13.2.1}{Regret}{subsection.13.2}% 76
\BOOKMARK [3][-]{subsubsection.13.2.2}{Lower Bound}{subsection.13.2}% 77
\BOOKMARK [3][-]{subsubsection.13.2.3}{EXP3}{subsection.13.2}% 78
